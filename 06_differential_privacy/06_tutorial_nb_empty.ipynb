{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514a7fca",
   "metadata": {},
   "source": [
    "## Tutorial for Differential Privacy\n",
    "\n",
    "In this tutorial, we will apply the differentially private stochastic graident descent algorithm for data-driven optimization. \n",
    "    \n",
    "As we have seen in the lecture, independent noise is added in iterations to achieve $(\\epsilon, \\delta)$-DP after $T$ iterations.\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b482a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sc\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ea7fe0",
   "metadata": {},
   "source": [
    "## Logistic regression model\n",
    "\n",
    "Our running example will be the logistic regression model, where the data is in the form \n",
    "\n",
    "$$\n",
    "x= (z, y),\n",
    "$$ \n",
    "where \n",
    "- $z \\in \\mathbb{R}^{d}$ is the feature vector\n",
    "- $y \\in \\{0, 1\\}$: binary response.\n",
    "\n",
    "\n",
    "The probability of observing a label ``1'' given the feature vector $z$ and regression parameter $\\theta \\in \\mathbb{R}^{d}$ is\n",
    "$$\n",
    "p(y | z, \\theta) = \\frac{e^{y z \\theta}}{1 + e^{z \\theta}}, \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10117c65",
   "metadata": {},
   "source": [
    "Below, we generate a training and a test set, of sizes $n_{\\text{train}}$ and $n_{\\text{test}}$, from a logistic regression model whose $d \\times 1$ true parameter vector is randomly generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b193db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the test and the train data\n",
    "n_train = 10000\n",
    "n_test = 10000\n",
    "n = n_train + n_test\n",
    "\n",
    "d = 20 # dimension\n",
    "B1 = 2*(d) # L1 sensitivity\n",
    "B2 = 2*np.sqrt(d) # L2 sensitivity\n",
    "\n",
    "theta_true = np.random.normal(size = d)\n",
    "\n",
    "# Generate data\n",
    "Z = np.concatenate((np.ones((n, 1)), np.random.uniform(size = (n, d-1))-0.5), axis = 1)\n",
    "Ztheta = np.matmul(Z, theta_true)\n",
    "p_true = np.exp(Ztheta)/(1 + np.exp(Ztheta))\n",
    "y = np.random.uniform(size = n) < p_true\n",
    "\n",
    "# Split to test-train\n",
    "Z_train = Z[:n_train]\n",
    "y_train = y[:n_train]\n",
    "\n",
    "Z_test = Z[n_train:]\n",
    "y_test = y[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e56dc",
   "metadata": {},
   "source": [
    "### Calculating the Gradient:\n",
    "\n",
    "Let\n",
    "$$\n",
    "f(\\theta; x) = -\\ln p(y | z, \\theta) = -\\ln e^{ y z \\theta}  + \\ln (1 + e^{z \\theta})\n",
    "$$\n",
    "\n",
    "Estimate $\\theta$ by minimizing\n",
    "$$\n",
    "F(\\theta; x_{1:n}) := \\frac{1}{n} \\sum_{i  = 1}^{n} f(\\theta; x_{i}) + \\lambda \\| \\theta \\|^{2}\n",
    "$$\n",
    "\n",
    "\n",
    "It can be shown that\n",
    "$$\n",
    "\\nabla f(\\theta; x) = - \\nabla \\ln p(y | z, \\theta) = -\\frac{z y}{1 + e^{\\theta z}} + z \\frac{e^{\\theta z}}{1 + e^{\\theta z}}\n",
    "$$\n",
    "\n",
    "The following functions calculate this gradient. The first function takes a single pair of $(z, y)$ and calculates the gradient at the given $\\theta$. The second function is a vectorized version of the first one, and it is much faster!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd85841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_grad(z, y, theta):\n",
    "    return -(z*y - z*np.exp(np.dot(theta,z))/(1 + np.exp(np.dot(theta,z))))\n",
    "\n",
    "def logistic_regression_grad_vec(Z, y, theta):\n",
    "    denom = (1 + np.exp(np.dot(Z, theta)))\n",
    "    temp = (np.exp(np.dot(Z, theta))/denom)\n",
    "    return -(Z*y[:, None] - Z*temp[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931cf610",
   "metadata": {},
   "source": [
    "### The main function: DP-SGD\n",
    "\n",
    "One iteration of the algorithm looks like\n",
    "    \n",
    "$$\n",
    "\\theta_{t+1} = \\theta_{t} - \\eta \\left( \\frac{1}{m_{t}}  \\sum_{i \\in U_{t}} \\nabla f_{i}(\\theta_{t}; x_{i}) + v_{t} \\right)\n",
    "$$\n",
    "\n",
    "The distribution of the DP noise $v_{t}$ depends on\n",
    "- DP parameters: $\\epsilon, \\delta$.\n",
    "- Sensitivity of $\\nabla f_{i}(\\theta_{t}; \\cdot)$\n",
    "- $m_{t}$ (privacy amplification by subsampling)\n",
    "- $T$ (composition property)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060f2a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DP_SGD(Z, y, lamda, eta, eps_DP, delta_DP, T, B1, B2, gamma):\n",
    "    \n",
    "    # Z, y are the dataset\n",
    "    # lamda is the regularization parameter\n",
    "    # eta is the step size\n",
    "    # eps_DP, delta_DP: DP parameters\n",
    "    # T: num of iterations\n",
    "    # B1, B2: L1- and L2- sensitivities of the gradient\n",
    "    # gamma: subsampling rate for SGD (in [0, 1]); the batch size is data size*gamma (closest integer)\n",
    "    \n",
    "    # Get the length of the data\n",
    "    [n, d] = Z.shape\n",
    "    \n",
    "    # check delta first. If delta > 0 and no subsampling, we will use the Gaussian mechanism,\n",
    "    # if delta = 0, we use Laplace mechanism\n",
    "    if delta_DP == 0:        \n",
    "        # We use Laplace noise \n",
    "        DP_noise_dist = \"Laplace\"\n",
    "        \n",
    "        # calculate the subsample size\n",
    "        m = int(n*gamma)\n",
    "        \n",
    "        # calculate the parameter of the Laplace noise\n",
    "        sigma_DP = B1/(m * np.log(1 + np.exp(eps_DP/T - 1)*n/m))\n",
    "        \n",
    "    else: #(epsilon, delta)-DP with no subsampling\n",
    "        # We use Gaussian noise \n",
    "        DP_noise_dist = \"Gauss\"\n",
    "        gamma == 1\n",
    "        rho_zCDP = np.sqrt(np.log(1/delta_DP) + eps_DP) - np.sqrt(np.log(1/delta_DP))\n",
    "        sigma_DP = np.sqrt(T*(B2**2)/((n**2)*(rho_zCDP**2)))\n",
    "        \n",
    "    \n",
    "    # DP SGD Iterations\n",
    "    theta = np.zeros(d)\n",
    "\n",
    "    Thetas = np.zeros((T, d))\n",
    "\n",
    "    for t in range(T):\n",
    "        # subsample (if needed)\n",
    "        if gamma < 1:\n",
    "            U = np.random.choice(n, m)\n",
    "        else:\n",
    "            U = np.arange(0, n)\n",
    "\n",
    "        # Calculate the stochastic gradient (over the subset)\n",
    "        # avg_grad = np.mean([logistic_regression_grad(Z[i], y[i], theta) for i in U], axis = 0)\n",
    "        avg_grad = np.mean(logistic_regression_grad_vec(Z[U], y[U], theta), axis = 0)\n",
    "\n",
    "        # generate the DP noise\n",
    "        if DP_noise_dist == \"Laplace\":\n",
    "            v = np.random.laplace(scale = sigma_DP, size = d)\n",
    "        elif DP_noise_dist == \"Gauss\":\n",
    "            v = np.random.normal(scale = sigma_DP,size = d)\n",
    "\n",
    "        # Make the update\n",
    "        theta = theta - eta*(avg_grad + lamda*theta + v)\n",
    "\n",
    "        Thetas[t] = theta\n",
    "\n",
    "    return Thetas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e90b9",
   "metadata": {},
   "source": [
    "## Experiments \n",
    "Let us do some experiments with several scenarios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a5aa59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scenarios [eps_DP, delta_DP, gamma, T, lamda, eta]\n",
    "Scenarios = [[np.inf, 0, 1, 1000, 0.001, 1], # Scenario 0: non-private\n",
    "                 [1, 0, 1, 1000, 0.001, 1],# Scenario 1a: (eps, 0)-DP (pure DP) without subs\n",
    "                 [1, 0, 0.1, 1000, 0.001, 1],# Scenario 1b: (eps, 0)-DP (pure DP) with subs\n",
    "                 [1, 1/n_train, 1, 1000, 0.001, 1], # Scenario 2a: (eps, delta)-DP without subs\n",
    "                 [0.5, 1/n_train, 1, 1000, 0.001, 1], # Scenario 2b: (eps, delta)-DP wo subs, more private\n",
    "                 [0.1, 1/n_train, 1, 1000, 0.001, 1], # Scenario 2c: (eps, delta)-DP wo subs, even more private\n",
    "                 [1, 1/n_train, 1, 10000, 0.001, 1], # Scenario 2d: (eps, delta)-DP wo subs, even more private, more iterations\n",
    "                 ]\n",
    "\n",
    "LC = len(Scenarios)\n",
    "\n",
    "Thetas = [None] * LC\n",
    "test_accuracy = np.zeros(LC)\n",
    "SE = [None] * LC\n",
    "\n",
    "for i in range(LC):\n",
    "    print(\"Experiment\", i, \"is running...\")\n",
    "    [eps_DP, delta_DP, gamma, T, lamda, eta] = Scenarios[i]\n",
    "    thetas = DP_SGD(Z_train, y_train, lamda, eta, eps_DP, delta_DP, T, B1, B2, gamma)\n",
    "    \n",
    "    # training error vs time\n",
    "    Est_error = thetas-theta_true\n",
    "    SE[i] = [np.linalg.norm(Est_error[t, :])**2 for t in range(T)]\n",
    "    \n",
    "    # prediction error for the outcomes \n",
    "    y_pred = np.matmul(Z_test, thetas[-1]) > 0\n",
    "    \n",
    "    test_accuracy[i] = sum(y_pred == y_test)/n_test\n",
    "    Thetas[i] = thetas\n",
    "    print(\"Test accuracy for Scenario\", i, \"with eps =\", eps_DP, \"delta =\", delta_DP, \"gamma=\", gamma, \"T=\", T, \"is\", test_accuracy[i])\n",
    "    \n",
    "print(\"Experiments are completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a22a50",
   "metadata": {},
   "source": [
    "The testing accuracies and running times may already give you some ideas. Here are some observations.\n",
    "\n",
    "- The best algorithm is the non-private one, which corresponds to $\\epsilon = \\infty$. \n",
    "- Subsampling boosted the algorithm in terms of speed, and there was not much difference with the no-subsamplnig case. \n",
    "        -- This is because, while subsampling possibly degrades the performance, it amplifies the privacy so requires less noise, which in turn compensates for the inaccuracy caused by subsampling.\n",
    "- Most expectedly, as epsilon gets smaller, we have worse performance.\n",
    "- Note what happens when we increase $T$. In general, after some value of $T$, the algorithm should degenerate because, due to the composition property, more iterations means more DP noise.  \n",
    "\n",
    "\n",
    "#### Caution:\n",
    "While interpreting these results, consider the randomness. We run each scenario only once. If we wanted to seriously compare between the scenarios, the randomness should be dealt by having multiple Monte Carlo runs for each scenario and look at the 'average' performance.\n",
    "\n",
    "\n",
    "   ### Now let us see the training performances, where the abovementioned effects are much more visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ad615c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(LC, 2)\n",
    "for i in range(LC):\n",
    "    \n",
    "             \n",
    "    ax[i, 0].plot(Thetas[i])\n",
    "    ax[i, 0].set_ylabel('iterates')\n",
    "    ax[i, 0].set_title('Theta iterates for Scenario %d' % (i),  fontsize=7)\n",
    "    ax[i, 0].set_xlabel('t')\n",
    "    \n",
    "    ax[i, 1].plot(SE[i])\n",
    "    ax[i, 1].set_ylabel('SE(t)')\n",
    "    ax[i, 1].set_xlabel('t')\n",
    "    ax[i, 1].set_title('Squared error for Scenario %d' % (i), fontsize=7)\n",
    "\n",
    "plt.subplots_adjust(left=0.1,\n",
    "                    bottom=0.1, \n",
    "                    right=0.9, \n",
    "                    top=0.9, \n",
    "                    wspace=0.4, \n",
    "                    hspace=1.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c75a20",
   "metadata": {},
   "source": [
    "Try this code with other configurations that you want to explore! For example, try and see what happens when you change \n",
    "- n, data size\n",
    "- d, dimension\n",
    "- subsampling rate\n",
    "- etc.\n",
    "\n",
    "Before whatever you try, try to make a guess on what kind of behaviour you will see. Then, check your guess against the numerical results.\n",
    "\n",
    "However, recall, again, that\n",
    "\n",
    "- the code, as it is, does not support the choice of $\\delta > 0$ WITH $\\gamma < 1$. However, the details of how to design an $\\epsilon, \\delta$-DP for that case is given in the lecture slides under the title \"Scenario 3\".\n",
    "- consider the randomness in the results due to performing a single Monte Carlo run for each scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39cb8d-4e71-4dc3-875d-1187abbf8feb",
   "metadata": {},
   "source": [
    "If you see any typos, you have any questions, you want to further discuss, etc, please contact me at\n",
    "sinanyildirim@sabanciuniv.edu "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trustworthy_ai_venv",
   "language": "python",
   "name": "trustworthy_ai_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
