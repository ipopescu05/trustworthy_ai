{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b749cc70",
   "metadata": {},
   "source": [
    "# Week 02 – Glassbox Models (part 1)\n",
    "\n",
    "In this notebook we'll be applying different glassbox models to the same dataset and compare their performance in terms of accuracy and f1-score and their interpretability.\n",
    "\n",
    "#### Step 1: Navigate to this week's directory \n",
    "```\n",
    "cd <path_to_week_2_material>\n",
    "```\n",
    "\n",
    "#### Step 2: Create and Activate a virtual environment (Python 3.9)\n",
    "**MacOS**\n",
    "```\n",
    "python3.9 -m venv part1_venv\n",
    "source part1_venv/bin/activate\n",
    "```\n",
    "\n",
    "**Windows (cmd)**\n",
    "```\n",
    "python3.9 -m venv part1_venv\n",
    "part1_venv\\Scripts\\activate\n",
    "```\n",
    "\n",
    "#### Step 3: Install required packages\n",
    "First, install `ipykernel` to integrate your virtual environment with Jupyter.\n",
    "```\n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=part1_venv\n",
    "```\n",
    "\n",
    "Next, install all necessary dependencies from the requirements.txt file.\n",
    "```\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "#### Step 4: Install `pydl8.5-lbguess`\n",
    "```\n",
    "git clone https://github.com/ubc-systopia/pydl8.5-lbguess.git\n",
    "cd pydl8.5-lbguess\n",
    "python3 setup.py install\n",
    "```\n",
    "\n",
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4b241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e93fa",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "We are using the [Titanic dataset](https://www.kaggle.com/c/titanic/overview), which holds data about passangers of the Titanic and whether they survived or not. Passengers are described by 7 features. The response variable is binary (0 – died; 1 - survived).\n",
    "\n",
    "Make sure to save the dataset in the parent directory or adjust the file path below.\n",
    "\n",
    "We are directly loading the pre-processed data sets. For some algorithms, we require the data to be in binary form. Hence, we have two versions of X: `X_train` and `X_test`, with continous features and one-hot encoded categorical features, and `X_train_bin` and `X_test_bin`, where all features have been one-hot encoded. For this, continuous features were first transformed into categories. Check the code in `./01_intro/titanic_data_prep.ipynb` for more details on pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248acf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train \n",
    "X_train = pd.read_csv('../datasets/titanic/encoded_titanic_X_train.csv')\n",
    "X_train_bin = pd.read_csv('../datasets/titanic/bin_titanic_X_train.csv')\n",
    "\n",
    "# X_test\n",
    "X_test = pd.read_csv('../datasets/titanic/encoded_titanic_X_test.csv')\n",
    "X_test_bin = pd.read_csv('../datasets/titanic/bin_titanic_X_test.csv')\n",
    "\n",
    "# y_train and y_test\n",
    "y_train = pd.read_csv('../datasets/titanic/titanic_y_train.csv')\n",
    "y_test = pd.read_csv('../datasets/titanic/titanic_y_test.csv')\n",
    "\n",
    "# take a look at the data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad58d6",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "- `Age` – age of a person in years (int)\n",
    "- `SibSp` – the number of siblings or spouse of a person **onboard** (int)\n",
    "- `Parch` – the number of parents or children of a person **onbard** (int)\n",
    "- `Fare` – ticket price (float)\n",
    "- `Sex` – sex of a person (categorical/binary)\n",
    "- `Embarked` – location where the traveler mounted from. There are three possible values — Southampton, Cherbourg, and Queenstown (categorical)\n",
    "- `Pclass` – passenger division into class 1, 2, and 3 (categorical)\n",
    "- `Survived` – whether person survived the sinking of the ship (binary). Less than 40% survived. This is the **outcome** to predict. \n",
    "\n",
    "\n",
    "From the original dataset and from preprocessing the data, we know the following about the **categorical features**:\n",
    "\n",
    "- `Sex` has two values `['female','male']`, which were encoded `[0,1]`, respectively. Then, after applying one-hot encoding, we have `Sex_1` which indicates `male` if 1, `female` otherwise.\n",
    "- `Embarked` has three values `['C', 'Q', 'S']`, which were encoded `[0,1,2]`, respectively. Hence,\n",
    "    - `Embarked_1 = 1` indicates `Q` \n",
    "    - `Embarked_2 = 1` indicates `S`\n",
    "    - `Embarked_1 = 0` and `Embarked_2 = 0` indicated `C`\n",
    "- `Pclass` has three values `[1,2,3]`, which were encoded `[0,1,2]`, respectively. Hence, after encoding, we have:\n",
    "    - `Pclass_1 = 1` indicates `2`\n",
    "    - `Pclass_2 = 1` indicates `3`\n",
    "    - `Pclass_1 = 0` and `Pclass_2 = 0` indicates `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e798643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "    'target': 'Survived',\n",
    "    'numerical':['Age', 'SibSp', 'Parch', 'Fare'],\n",
    "    'categorical':['Sex', 'Embarked', 'Pclass']\n",
    "}\n",
    "\n",
    "# save feature names\n",
    "feature_names = X_train.columns\n",
    "target_names = list(y_train[d['target']].unique())\n",
    "\n",
    "# create a dataframe to save y values in \n",
    "y_results = pd.DataFrame()\n",
    "y_results['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60564ca",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e7935",
   "metadata": {},
   "source": [
    "# 1. Linear Models – Logistic Regression\n",
    "\n",
    "The most commonly used package to run a logistic or linear regression model is `scikit-learn`. However, the package `interpret` has a wrapper for the logistic regression model fit by `scikit-learn`, with the added value of visualizing the model and for purposes of (global and local) interpretation. Hence, we will be using `interpret` to fit the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fafb9e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "from interpret.glassbox import LogisticRegression\n",
    "from interpret import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71ca2701",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=0)\n",
    "lr.fit(X_train,y_train)\n",
    "y_results['lr_pred'] = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda621c5",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef9693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['lr_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['lr_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['lr_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443527a",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 1.1 – Evaluate the model performance.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f606236",
   "metadata": {},
   "source": [
    "## model interpretation\n",
    "---\n",
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64b2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_global_expl = lr.explain_global()\n",
    "show(lr_global_expl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19495326",
   "metadata": {},
   "source": [
    "We can also inspect the feature contribution of each (or the x most important) feature(s) and/or interactions of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12afc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_dict = lr_global_expl.data()\n",
    "\n",
    "# combine names and scores into tuples\n",
    "name_score_pairs = list(zip(expl_dict['names'], expl_dict['scores']))\n",
    "\n",
    "# sort by scores in descending order\n",
    "sorted_name_score_pairs = sorted(name_score_pairs, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# print the sorted names, scores, and ranks\n",
    "for rank, (name, score) in enumerate(sorted_name_score_pairs, start=1):\n",
    "    print(f'{rank}. {name}: {round(score,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f8d888",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 1.2 – Explain what is shown in the summary plot.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0932183f",
   "metadata": {},
   "source": [
    "---\n",
    "### LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9f73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(lr.explain_local(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9bbb92",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 1.3 – Take a look at the local explanation for sample with index 2 of the test set. How do you explain that `Age` seems to be much more influential in the prediction for this instance, while it does receives almost no importance in the summary plot?**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f073c1",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d100a6",
   "metadata": {},
   "source": [
    "# 2. Generalized Additive Models (GAM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5548996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "from pygam import LogisticGAM, s, f, l, te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e611af",
   "metadata": {},
   "source": [
    "If you have a lot of features in the dataset, you may only want to pick some of the features to include in the model. For example, for each feature, we can check whether there is a significant correlation with the dependent variable. For categorical features, we use Chi-square; for numerical features we use Spearman's rank correlation. Then, we could select those where the associated p-value is below 0.05. However, here we will include all variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1fb47",
   "metadata": {},
   "source": [
    "Typically, we use factor terms `f()` for categorical variables and spline terms `s()` for numerical variables. We can use linear terms `l()` for numerical variables with a linear relationship to the response variable. Lastly, we could also incluse interaction terms `te()` between two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cbab803",
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = LogisticGAM(s(0) + s(1) + s(2) + s(3) + f(4) + f(5) + f(6) + f(7) + f(8))\n",
    "gam.fit(X_train, y_train)\n",
    "\n",
    "y_results['gam_pred'] = gam.predict(X_test)*1 # *1 to concert to 0's and 1's (from True and False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e38b97",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee551382",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['gam_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['gam_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['gam_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f64abcb",
   "metadata": {},
   "source": [
    "## model interpretation\n",
    "---\n",
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e2df74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gam.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbaa75",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 3. Explainable Boosting Machine (EBM)\n",
    "\n",
    "Explainable Boosting Machine (EBM) has been implemented by interpretML. See the documentation here: https://interpret.ml/docs/python/api/ExplainableBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34923c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "from interpret import show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6aa577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = ExplainableBoostingClassifier()\n",
    "ebm.fit(X_train, y_train)\n",
    "y_results['ebm_pred'] = ebm.predict(X_test) #save predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15764b36",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9329d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['ebm_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['ebm_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['ebm_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0e83bf",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 3.1 – Evaluate the performance of this model by taking into account the performance scores.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365c3041",
   "metadata": {},
   "source": [
    "## model interpretation\n",
    "\n",
    "---\n",
    "### GLOBAL\n",
    "\n",
    "In the display below, you can see the all feature importances in a summary plot and you can select each feature separately and inspect it's impact on the predictions more closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af325820",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ebm_global_expl = ebm.explain_global()\n",
    "show(ebm_global_expl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b9e177",
   "metadata": {},
   "source": [
    "We can also inspect the feature contribution of each (or the x most important) feature(s) and/or interactions of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d40f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expl_dict = ebm_global_expl.data()\n",
    "\n",
    "# combine names and scores into tuples\n",
    "name_score_pairs = list(zip(expl_dict['names'], expl_dict['scores']))\n",
    "\n",
    "# sort by scores in descending order\n",
    "sorted_name_score_pairs = sorted(name_score_pairs, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# print the sorted names, scores, and ranks\n",
    "for rank, (name, score) in enumerate(sorted_name_score_pairs[:10], start=1):\n",
    "    print(f'{rank}. {name}: {round(score,4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e63cdd9",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 3.2 – Explain how the feature importances in the summary plot are obtained.**\n",
    "\n",
    "...\n",
    "\n",
    "**Q 3.3 – How can we interpret the error bars in the plots of individual features?**\n",
    "\n",
    "...\n",
    "\n",
    "**Q 3.4 – Provide a full interpretation of the importance of feature `credit_amount` using these plots.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bae6e45",
   "metadata": {},
   "source": [
    "---\n",
    "### LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae54984",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(ebm.explain_local(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1ebd44",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 3.5 – Take a look at the local explanation for sample with index 9 of the test set. How would you interpret this explanation?**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7b44b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 4. Decision Trees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb81b6",
   "metadata": {},
   "source": [
    "## 4.1 Fast Sparse Decision Trees\n",
    "\n",
    "This is a developing field and many people publish their code on Github. We will be using the code from this Github repo: https://github.com/ubc-systopia/pydl8.5-lbguess, which provides the code for the paper by [McTavish et al. (2022)](https://arxiv.org/abs/2112.00798) on Fast Sparse Decision Trees.\n",
    "\n",
    "Note: This algorithm works with **binary** input data only. \n",
    "\n",
    "⚠️ This code requires `scikit-learn==1.1.2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6f17ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(1, os.path.dirname(os.getcwd())+'/pydl8.5-lbguess')\n",
    "from dl85 import DL85Classifier\n",
    "import fsdt_helpers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50cca2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsdt = DL85Classifier(time_limit=300, desc=True, max_depth=3)\n",
    "fsdt.fit(X_train_bin, y_train)\n",
    "y_results['fsdt_pred'] = fsdt.predict(X_test_bin) #save predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f617c8b",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecd86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['fsdt_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['fsdt_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['fsdt_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ab1755",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 4.1.1 – Evaluate the performance of this model by taking into account the performance scores.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c618e",
   "metadata": {},
   "source": [
    "## model interpretation\n",
    "\n",
    "We wrote some helper functions to extract the number of leaves (aka rules), the average rule length in the tree, and the average rule length per sample.\n",
    "\n",
    "---\n",
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a769821",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rules = %.0f' % fsdt_helpers.get_num_leaves(fsdt.tree_))\n",
    "print('Average rule length = %.2f' % fsdt_helpers.get_avg_rule_length(fsdt.tree_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c723ce96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot tree\n",
    "fsdt_helpers.display_tree(tree = fsdt.tree_, n = len(X_train_bin), feature_names = X_train_bin.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f246ca",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 4.1.2 – Discuss global interpretability of the resulting tree.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5f27de",
   "metadata": {},
   "source": [
    "---\n",
    "### LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7e18bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average number of rules used per sample = %.2f' % 1.00)\n",
    "print('Average rule length per sample = %.2f' % fsdt_helpers.get_avg_rule_length_per_sample(fsdt.tree_,np.array(X_test_bin)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85175633",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_bin.iloc[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca885a",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 4.1.3 – Explain how you would make the prediction for sample with index 2 of the test set.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161a223",
   "metadata": {},
   "source": [
    "## 4.2 CART\n",
    "\n",
    "Let's compare this to the most popular algorithm to grow a decision tree, which is called CART and is implemented in `scikit-learn`. \n",
    "\n",
    "To be able to **directly** compare both trees, we will again use the **binary** datasets `X_train_bin` and `X_test_bin` to train and test the model, respectively, even though CART can handle continous data.\n",
    "\n",
    "Feel free to train CART on the continous data `X_train` and inspect the results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab856189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de66bf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(max_depth=3)\n",
    "tree.fit(X_train_bin, y_train)\n",
    "y_results['tree_pred'] = tree.predict(X_test_bin) #save predicted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f5b94b",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df45c027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['tree_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['tree_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['tree_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c49259f",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 4.2.1 – Evaluate the performance of this model by taking into account the performance scores.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3408911",
   "metadata": {},
   "source": [
    "## model interpretation \n",
    "---\n",
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ec4ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,15))\n",
    "_ = plot_tree(tree, feature_names=X_train_bin.columns, class_names=['{:.0f}'.format(x) for x in target_names], filled=True, fontsize=15)\n",
    "\n",
    "print('Number of rules = %.0f' % tree.get_n_leaves())\n",
    "print('Average rule length = %.2f' % tree.get_depth())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1d1815",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 4.2.2 – Which feature is, based on this tree, the most important variable to predict survival?**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54ea5c8",
   "metadata": {},
   "source": [
    "--- \n",
    "### LOCAL\n",
    "\n",
    "### Questions\n",
    "\n",
    "**Q 4.2.3 – Explain how you would make the prediction for sample with index 2 of the test set.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eff623",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b732c14",
   "metadata": {},
   "source": [
    "# 5. Decision Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26cc733",
   "metadata": {},
   "source": [
    "## 5.1 Decision Lists\n",
    "\n",
    "We will use another algorithm that generates rules and is implemented in `interpret`. See the documentation here: https://interpret.ml/docs/dr.html. \n",
    "\n",
    "The package uses a wrapper for the algorithm skope-rules from [Gardin et al. (2017)](https://zenodo.org/records/4316671), which is a weighted combination of rules extracted from a tree ensemble using L1-regularized optimization over the weights.\n",
    "\n",
    "Typically, in a decision list, the prediction is made using only the first rule in the list that applies to a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e70463c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret.glassbox import DecisionListClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf1968db",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DecisionListClassifier(random_state=21, max_depth=3)\n",
    "dl.fit(X_train, y_train)\n",
    "y_results['dl_pred'] = dl.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667c7bf5",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83f8848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['dl_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['dl_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['dl_pred']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe33680",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 5.1.1 – Evaluate the model performance.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0f3f2e",
   "metadata": {},
   "source": [
    "## model interpretation\n",
    "---\n",
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "670a3746",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_global_expl = dl.explain_global()\n",
    "expl_dict = dl_global_expl.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c435b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of rules = %.0f' % (len(dl.rules_)-1))\n",
    "print('Average rule length = %.2f' % np.mean([r.count('and') for r in expl_dict['rule'][:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d28ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(dl_global_expl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28a40d",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 5.1.2 – Evaluate the global interpretability of the model.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4db1ea",
   "metadata": {},
   "source": [
    "---\n",
    "### LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362f48ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "show(dl.explain_local(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "part1_venv",
   "language": "python",
   "name": "part1_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
