
@article{rudin2019_stopexplaining,
	abstract = {Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision.},
	author = {Rudin, Cynthia},
	date = {2019/05/01},
	date-added = {2023-12-05 14:59:19 +0100},
	date-modified = {2023-12-05 14:59:19 +0100},
	doi = {10.1038/s42256-019-0048-x},
	id = {Rudin2019},
	isbn = {2522-5839},
	journal = {Nature Machine Intelligence},
	number = {5},
	pages = {206--215},
	title = {Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead},
	url = {https://doi.org/10.1038/s42256-019-0048-x},
	volume = {1},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s42256-019-0048-x}}



@article{abdullah2021_review,
	abstract = {We have witnessed the impact of ML in disease diagnosis, image recognition and classification, and many more related fields. Healthcare is a sensitive field related to people&rsquo;s lives in which decisions need to be carefully taken based on solid evidence. However, most ML models are complex, i.e., black-box, meaning they do not provide insights into how the problems are solved or why such decisions are proposed. This lack of interpretability is the main reason why some ML models are not widely used yet in real environments such as healthcare. Therefore, it would be beneficial if ML models could provide explanations allowing physicians to make data-driven decisions that lead to higher quality service. Recently, several efforts have been made in proposing interpretable machine learning models to become more convenient and applicable in real environments. This paper aims to provide a comprehensive survey and symmetry phenomena of IML models and their applications in healthcare. The fundamental characteristics, theoretical underpinnings needed to develop IML, and taxonomy for IML are presented. Several examples of how they are applied in healthcare are investigated to encourage and facilitate the use of IML models in healthcare. Furthermore, current limitations, challenges, and future directions that might impact applying ML in healthcare are addressed.},
	article-number = {2439},
	author = {Abdullah, Talal A. A. and Zahid, Mohd Soperi Mohd and Ali, Waleed},
	doi = {10.3390/sym13122439},
	issn = {2073-8994},
	journal = {Symmetry},
	number = {12},
	title = {A Review of Interpretable {ML} in Healthcare: Taxonomy, Applications, Challenges, and Future Directions},
	url = {https://www.mdpi.com/2073-8994/13/12/2439},
	volume = {13},
	year = {2021},
	bdsk-url-1 = {https://www.mdpi.com/2073-8994/13/12/2439},
	bdsk-url-2 = {https://doi.org/10.3390/sym13122439}}

@INPROCEEDINGS{das2020_review,
  author={Das, Saikat and Agarwal, Namita and Venugopal, Deepak and Sheldon, Frederick T. and Shiva, Sajjan},
  booktitle={2020 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
  title={Taxonomy and Survey of Interpretable Machine Learning Method}, 
  year={2020},
  volume={},
  number={},
  pages={670-677},
  doi={10.1109/SSCI47803.2020.9308404},
  url = {https://doi.org/10.1109/SSCI47803.2020.9308404}}

@INPROCEEDINGS{gilpin2018_review,
  author={Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  booktitle={2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)}, 
  title={Explaining Explanations: An Overview of Interpretability of Machine Learning}, 
  year={2018},
  volume={},
  number={},
  pages={80-89},
  doi={10.1109/DSAA.2018.00018},
  url={https://doi.org/10.1109/DSAA.2018.00018}}


@article{murdoch2019_review,
	abstract = {Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.},
	author = {W. James Murdoch and Chandan Singh and Karl Kumbier and Reza Abbasi-Asl and Bin Yu},
	doi = {10.1073/pnas.1900654116},
	eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.1900654116},
	journal = {Proceedings of the National Academy of Sciences},
	number = {44},
	pages = {22071-22080},
	title = {Definitions, methods, and applications in interpretable machine learning},
	url = {https://www.pnas.org/doi/abs/10.1073/pnas.1900654116},
	volume = {116},
	year = {2019},
	bdsk-url-1 = {https://www.pnas.org/doi/abs/10.1073/pnas.1900654116},
	bdsk-url-2 = {https://doi.org/10.1073/pnas.1900654116}}

@ARTICLE{tjoa2021_review,
  author={Tjoa, Erico and Guan, Cuntai},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={A Survey on Explainable Artificial Intelligence ({XAI}): Toward Medical {XAI}}, 
  year={2021},
  volume={32},
  number={11},
  pages={4793-4813},
  doi={10.1109/TNNLS.2020.3027314},
url={https://doi.org/10.1109/TNNLS.2020.3027314}}

@INPROCEEDINGS{shailaja2018_review,
  author={Shailaja, K. and Seetharamulu, B. and Jabbar, M. A.},
  booktitle={2018 Second International Conference on Electronics, Communication and Aerospace Technology (ICECA)}, 
  title={Machine Learning in Healthcare: A Review}, 
  year={2018},
  volume={},
  number={},
  pages={910-914},
  doi={10.1109/ICECA.2018.8474918}}


@article{ras2022explainable,
  title={Explainable deep learning: A field guide for the uninitiated},
  author={Ras, Gabrielle and Xie, Ning and Van Gerven, Marcel and Doran, Derek},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={329--396},
  year={2022},
  url={https://doi.org/10.1613/jair.1.13200},
  doi={10.1613/jair.1.13200}
}


@article{markus2021role,
	abstract = {Artificial intelligence (AI) has huge potential to improve the health and well-being of people, but adoption in clinical practice is still limited. Lack of transparency is identified as one of the main barriers to implementation, as clinicians should be confident the AI system can be trusted. Explainable AI has the potential to overcome this issue and can be a step towards trustworthy AI. In this paper we review the recent literature to provide guidance to researchers and practitioners on the design of explainable AI systems for the health-care domain and contribute to formalization of the field of explainable AI. We argue the reason to demand explainability determines what should be explained as this determines the relative importance of the properties of explainability (i.e. interpretability and fidelity). Based on this, we propose a framework to guide the choice between classes of explainable AI methods (explainable modelling versus post-hoc explanation; model-based, attribution-based, or example-based explanations; global and local explanations). Furthermore, we find that quantitative evaluation metrics, which are important for objective standardized evaluation, are still lacking for some properties (e.g. clarity) and types of explanations (e.g. example-based methods). We conclude that explainable modelling can contribute to trustworthy AI, but the benefits of explainability still need to be proven in practice and complementary measures might be needed to create trustworthy AI in health care (e.g. reporting data quality, performing extensive (external) validation, and regulation).},
	author = {Aniek F. Markus and Jan A. Kors and Peter R. Rijnbeek},
	doi = {https://doi.org/10.1016/j.jbi.2020.103655},
	issn = {1532-0464},
	journal = {Journal of Biomedical Informatics},
	keywords = {Explainable artificial intelligence, Trustworthy artificial intelligence, Interpretability, Explainable modelling, Post-hoc explanation},
	pages = {103655},
	title = {The role of explainability in creating trustworthy artificial intelligence for health care: A comprehensive survey of the terminology, design choices, and evaluation strategies},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
	volume = {113},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1532046420302835},
	bdsk-url-2 = {https://doi.org/10.1016/j.jbi.2020.103655}}


@article{markus2021role-old,
  title={The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies},
  author={Markus, Aniek F and Kors, Jan A and Rijnbeek, Peter R},
  journal={Journal of Biomedical Informatics},
  volume={113},
  pages={103655},
  year={2021},
  publisher={Elsevier}
}

@article{amann2020explainability-old,
  title={Explainability for artificial intelligence in healthcare: a multidisciplinary perspective},
  author={Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I and Precise4Q Consortium},
  journal={BMC Medical Informatics and Decision Making},
  volume={20},
  pages={1--9},
  year={2020},
  publisher={Springer}
}



@article{amann2020explainability,
	abstract = {Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.},
	author = {Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I. and the Precise4Q consortium},
	date = {2020/11/30},
	date-added = {2024-09-11 20:06:43 +0200},
	date-modified = {2024-09-11 20:06:43 +0200},
	doi = {10.1186/s12911-020-01332-6},
	id = {Amann2020},
	isbn = {1472-6947},
	journal = {BMC Medical Informatics and Decision Making},
	number = {1},
	pages = {310},
	title = {Explainability for artificial intelligence in healthcare: a multidisciplinary perspective},
	url = {https://doi.org/10.1186/s12911-020-01332-6},
	volume = {20},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1186/s12911-020-01332-6}}






@article{linardatos2021_review,
	abstract = {Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into &ldquo;black box&rdquo; approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.},
	article-number = {18},
	author = {Linardatos, Pantelis and Papastefanopoulos, Vasilis and Kotsiantis, Sotiris},
	doi = {10.3390/e23010018},
	issn = {1099-4300},
	journal = {Entropy},
	number = {1},
	pubmedid = {33375658},
	title = {Explainable AI: A Review of Machine Learning Interpretability Methods},
	url = {https://www.mdpi.com/1099-4300/23/1/18},
	volume = {23},
	year = {2021},
	bdsk-url-1 = {https://www.mdpi.com/1099-4300/23/1/18},
	bdsk-url-2 = {https://doi.org/10.3390/e23010018}}

@article{guidotti2018_review,
author = {Guidotti, Riccardo and Monreale, Anna and Ruggieri, Salvatore and Turini, Franco and Giannotti, Fosca and Pedreschi, Dino},
title = {A Survey of Methods for Explaining Black Box Models},
year = {2018},
issue_date = {September 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3236009},
doi = {10.1145/3236009},
abstract = {In recent years, many accurate decision support systems have been constructed as black boxes, that is as systems that hide their internal logic to the user. This lack of explanation constitutes both a practical and an ethical issue. The literature reports many approaches aimed at overcoming this crucial weakness, sometimes at the cost of sacrificing accuracy for interpretability. The applications in which black box decision systems can be used are various, and each approach is typically developed to provide a solution for a specific problem and, as a consequence, it explicitly or implicitly delineates its own definition of interpretability and explanation. The aim of this article is to provide a classification of the main problems addressed in the literature with respect to the notion of explanation and the type of black box system. Given a problem definition, a black box type, and a desired explanation, this survey should help the researcher to find the proposals more useful for his own work. The proposed classification of approaches to open black box models should also be useful for putting the many research open questions in perspective.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {93},
numpages = {42},
keywords = {explanations, interpretability, transparent models, Open the black box}
}


@article{carvalho2019_review,
	abstract = {Machine learning systems are becoming increasingly ubiquitous. These systems&rsquo;s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.},
	article-number = {832},
	author = {Carvalho, Diogo V. and Pereira, Eduardo M. and Cardoso, Jaime S.},
	doi = {10.3390/electronics8080832},
	issn = {2079-9292},
	journal = {Electronics},
	number = {8},
	title = {Machine Learning Interpretability: A Survey on Methods and Metrics},
	url = {https://www.mdpi.com/2079-9292/8/8/832},
	volume = {8},
	year = {2019},
	bdsk-url-1 = {https://www.mdpi.com/2079-9292/8/8/832},
	bdsk-url-2 = {https://doi.org/10.3390/electronics8080832}}



@article{barredo-arrieta2020_review,
	abstract = {In the last few years, Artificial Intelligence (AI) has achieved a notable momentum that, if harnessed appropriately, may deliver the best of expectations over many application sectors across the field. For this to occur shortly in Machine Learning, the entire community stands in front of the barrier of explainability, an inherent problem of the latest techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI (namely, expert systems and rule based models). Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is widely acknowledged as a crucial feature for the practical deployment of AI models. The overview presented in this article examines the existing literature and contributions already done in the field of XAI, including a prospect toward what is yet to be reached. For this purpose we summarize previous efforts made to define explainability in Machine Learning, establishing a novel definition of explainable Machine Learning that covers such prior conceptual propositions with a major focus on the audience for which the explainability is sought. Departing from this definition, we propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at explaining Deep Learning methods for which a second dedicated taxonomy is built and examined in detail. This critical literature analysis serves as the motivating background for a series of challenges faced by XAI, such as the interesting crossroads of data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to the field of XAI with a thorough taxonomy that can serve as reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	author = {Alejandro {Barredo Arrieta} and Natalia D{\'\i}az-Rodr{\'\i}guez and Javier {Del Ser} and Adrien Bennetot and Siham Tabik and Alberto Barbado and Salvador Garcia and Sergio Gil-Lopez and Daniel Molina and Richard Benjamins and Raja Chatila and Francisco Herrera},
	doi = {https://doi.org/10.1016/j.inffus.2019.12.012},
	issn = {1566-2535},
	journal = {Information Fusion},
	keywords = {Explainable Artificial Intelligence, Machine Learning, Deep Learning, Data Fusion, Interpretability, Comprehensibility, Transparency, Privacy, Fairness, Accountability, Responsible Artificial Intelligence},
	pages = {82-115},
	title = {Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
	volume = {58},
	year = {2020},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1566253519308103},
	bdsk-url-2 = {https://doi.org/10.1016/j.inffus.2019.12.012}}



@article{antoniadi2021_review,
	abstract = {Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.},
	article-number = {5088},
	author = {Antoniadi, Anna Markella and Du, Yuhan and Guendouz, Yasmine and Wei, Lan and Mazo, Claudia and Becker, Brett A. and Mooney, Catherine},
	doi = {10.3390/app11115088},
	issn = {2076-3417},
	journal = {Applied Sciences},
	number = {11},
	title = {Current Challenges and Future Opportunities for {XAI} in Machine Learning-Based Clinical Decision Support Systems: A Systematic Review},
	url = {https://www.mdpi.com/2076-3417/11/11/5088},
	volume = {11},
	year = {2021},
	bdsk-url-1 = {https://www.mdpi.com/2076-3417/11/11/5088},
	bdsk-url-2 = {https://doi.org/10.3390/app11115088}}



@article{ali2023_review,
	abstract = {Artificial intelligence (AI) is currently being utilized in a wide range of sophisticated applications, but the outcomes of many AI models are challenging to comprehend and trust due to their black-box nature. Usually, it is essential to understand the reasoning behind an AI model's decision-making. Thus, the need for eXplainable AI (XAI) methods for improving trust in AI models has arisen. XAI has become a popular research subject within the AI field in recent years. Existing survey papers have tackled the concepts of XAI, its general terms, and post-hoc explainability methods but there have not been any reviews that have looked at the assessment methods, available tools, XAI datasets, and other related aspects. Therefore, in this comprehensive study, we provide readers with an overview of the current research and trends in this rapidly emerging area with a case study example. The study starts by explaining the background of XAI, common definitions, and summarizing recently proposed techniques in XAI for supervised machine learning. The review divides XAI techniques into four axes using a hierarchical categorization system: (i) data explainability, (ii) model explainability, (iii) post-hoc explainability, and (iv) assessment of explanations. We also introduce available evaluation metrics as well as open-source packages and datasets with future research directions. Then, the significance of explainability in terms of legal demands, user viewpoints, and application orientation is outlined, termed as XAI concerns. This paper advocates for tailoring explanation content to specific user types. An examination of XAI techniques and evaluation was conducted by looking at 410 critical articles, published between January 2016 and October 2022, in reputed journals and using a wide range of research databases as a source of information. The article is aimed at XAI researchers who are interested in making their AI models more trustworthy, as well as towards researchers from other disciplines who are looking for effective XAI methods to complete tasks with confidence while communicating meaning from data.},
	author = {Sajid Ali and Tamer Abuhmed and Shaker El-Sappagh and Khan Muhammad and Jose M. Alonso-Moral and Roberto Confalonieri and Riccardo Guidotti and Javier {Del Ser} and Natalia D{\'\i}az-Rodr{\'\i}guez and Francisco Herrera},
	doi = {https://doi.org/10.1016/j.inffus.2023.101805},
	issn = {1566-2535},
	journal = {Information Fusion},
	keywords = {Explainable Artificial Intelligence, Interpretable machine learning, Trustworthy AI, AI principles, Post-hoc explainability, XAI assessment, Data Fusion, Deep Learning},
	pages = {101805},
	title = {Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
	volume = {99},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1566253523001148},
	bdsk-url-2 = {https://doi.org/10.1016/j.inffus.2023.101805}}



@inproceedings{Lundberg-SHAP,
	author = {Lundberg, Scott M and Lee, Su-In},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	publisher = {Curran Associates, Inc.},
	title = {A Unified Approach to Interpreting Model Predictions},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf},
	volume = {30},
	year = {2017},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper_files/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf}}

@inproceedings{Ribeiro2016-LIME,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {interpretable machine learning, interpretability, explaining machine learning, black box classifier},
location = {San Francisco, California, USA},
series = {KDD '16}
}


@article{wachter2017counterfactual,
  title={Counterfactual explanations without opening the black box: Automated decisions and the {GDPR}},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Harvard Journal of Law \& Technology},
  volume={31},
  pages={841},
  year={2017},
  publisher={HeinOnline}
}


@article{Russell.2019-old, 
year = {2019}, 
keywords = {counterfactuals}, 
title = {{Efficient search for diverse coherent explanations}}, 
author = {Russell, Chris}, 
journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {20--28}
}

@inproceedings{Russell.2019,
author = {Russell, Chris},
title = {Efficient Search for Diverse Coherent Explanations},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287569},
doi = {10.1145/3287560.3287569},
abstract = {This paper proposes new search algorithms for counterfactual explanations based upon mixed integer programming. We are concerned with complex data in which variables may take any value from a contiguous range or an additional set of discrete states. We propose a novel set of constraints that we refer to as a "mixed polytope" and show how this can be used with an integer programming solver to efficiently find coherent counterfactual explanations i.e. solutions that are guaranteed to map back onto the underlying data structure, while avoiding the need for brute-force enumeration. We also look at the problem of diverse explanations and show how these can be generated within our framework.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {20–28},
numpages = {9},
keywords = {Counterfactual Explanation, Linear Program, Machine Learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{Ustun.2019-old, 
year = {2019}, 
keywords = {counterfactuals}, 
title = {{Actionable recourse in linear classification}}, 
author = {Ustun, Berk and Spangher, Alexander and Liu, Yang}, 
journal = {Proceedings of the Conference on Fairness, Accountability, and Transparency}, 
eprint = {1809.06514}, 
abstract = {{Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood. In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.}}, 
pages = {10--19}}

@inproceedings{Ustun.2019,
author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
title = {Actionable Recourse in Linear Classification},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287566},
doi = {10.1145/3287560.3287566},
abstract = {Classification models are often used to make decisions that affect humans: whether to approve a loan application, extend a job offer, or provide insurance. In such applications, individuals should have the ability to change the decision of the model. When a person is denied a loan by a credit scoring model, for example, they should be able to change the input variables of the model in a way that will guarantee approval. Otherwise, this person will be denied the loan so long as the model is deployed, and -- more importantly --will lack agency over a decision that affects their livelihood.In this paper, we propose to evaluate a linear classification model in terms of recourse, which we define as the ability of a person to change the decision of the model through actionable input variables (e.g., income vs. age or marital status). We present an integer programming toolkit to: (i) measure the feasibility and difficulty of recourse in a target population; and (ii) generate a list of actionable changes for a person to obtain a desired outcome. We discuss how our tools can inform different stakeholders by using them to audit recourse for credit scoring models built with real-world datasets. Our results illustrate how recourse can be significantly affected by common modeling practices, and motivate the need to evaluate recourse in algorithmic decision-making.},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {10–19},
numpages = {10},
keywords = {accountability, audit, classification, credit scoring, integer programming, recourse},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@article{Kanamori.2020jn, 
    title={Ordered Counterfactual Explanation by Mixed-Integer Linear Optimization}, 
    volume={35}, 
    url={https://ojs.aaai.org/index.php/AAAI/article/view/17376}, 
    abstractNote={Post-hoc explanation methods for machine learning models have been widely used to support decision-making. One of the popular methods is Counterfactual Explanation (CE), also known as Actionable Recourse, which provides a user with a perturbation vector of features that alters the prediction result. Given a perturbation vector, a user can interpret it as an &quot;action&quot; for obtaining one’s desired decision result. In practice, however, showing only a perturbation vector is often insufficient for users to execute the action. The reason is that if there is an asymmetric interaction among features, such as causality, the total cost of the action is expected to depend on the order of changing features. Therefore, practical CE methods are required to provide an appropriate order of changing features in addition to a perturbation vector. For this purpose, we propose a new framework called Ordered Counterfactual Explanation (OrdCE). We introduce a new objective function that evaluates a pair of an action and an order based on feature interaction. To extract an optimal pair, we propose a mixed-integer linear optimization approach with our objective function. Numerical experiments on real datasets demonstrated the effectiveness of our OrdCE in comparison with unordered CE methods.}, 
    number={13}, 
    journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken and Ike, Yuichi and Uemura, Kento and Arimura, Hiroki}, 
    year={2021}, 
    month={May}, 
    pages={11564-11574} 
}


@inproceedings{Kanamori.2020,
	author = {Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken and Arimura, Hiroki},
	booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, {IJCAI-20}},
	doi = {10.24963/ijcai.2020/395},
	editor = {Christian Bessiere},
	month = {7},
	pages = {2855--2862},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {DACE: Distribution-Aware Counterfactual Explanation by Mixed-Integer Linear Optimization},
	url = {https://doi.org/10.24963/ijcai.2020/395},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2020/395}}


@inproceedings{Kanamori.2020-old,
author = {Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken and Arimura, Hiroki},
title = {DACE: distribution-aware counterfactual explanation by mixed-integer linear optimization},
year = {2021},
isbn = {9780999241165},
abstract = {Counterfactual Explanation (CE) is one of the posthoc explanation methods that provides a perturbation vector so as to alter the prediction result obtained from a classifier. Users can directly interpret the perturbation as an "action" for obtaining their desired decision results. However, an action extracted by existing methods often becomes unrealistic for users because they do not adequately care about the characteristics corresponding to the empirical data distribution such as feature-correlations and outlier risk. To suggest an executable action for users, we propose a new framework of CE for extracting an action by evaluating its reality on the empirical data distribution. The key idea of our proposed method is to define a new cost function based on the Mahalanobis' distance and the local outlier factor. Then, we propose a mixed-integer linear optimization approach to extracting an optimal action by minimizing our cost function. By experiments on real datasets, we confirm the effectiveness of our method in comparison with existing methods for CE.},
booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
articleno = {395},
numpages = {8},
location = {Yokohama, Yokohama, Japan},
series = {IJCAI'20}
}

@article{Kanamori.2020-old, 
year = {2020}, 
keywords = {counterfactuals}, 
title = {{DACE: distribution-aware counterfactual explanation by mixed-integer linear optimization}}, 
author = {Kanamori, Kentaro and Takagi, Takuya and Kobayashi, Ken and Arimura, Hiroki}, 
journal = {Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence},
abstract = {{Counterfactual Explanation (CE) is one of the post-hoc explanation methods that provides a perturbation vector so as to alter the prediction result obtained from a classifier. Users can directly interpret the perturbation as an "action" for obtaining their desired decision results. However, an action extracted by existing methods often becomes unrealistic for users because they do not adequately care about the characteristics corresponding to the empirical data distribution such as feature-correlations and outlier risk. To suggest an executable action for users, we propose a new framework of CE for extracting an action by evaluating its reality on the empirical data distribution. The key idea of our proposed method is to define a new cost function based on the Mahalanobis' distance and the local outlier factor. Then, we propose a mixed-integer linear optimization approach to extracting an optimal action by minimizing our cost function. By experiments on real datasets, we confirm the effectiveness of our method in comparison with existing methods for CE.}}, 
pages = {2855--2862}
}

@inproceedings{Karimi.2020,
author = {Karimi, Amir-Hossein and Sch\"{o}lkopf, Bernhard and Valera, Isabel},
title = {Algorithmic Recourse: From Counterfactual Explanations to Interventions},
year = {2021},
isbn = {9781450383097},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442188.3445899},
doi = {10.1145/3442188.3445899},
abstract = {As machine learning is increasingly used to inform consequential decision-making (e.g., pre-trial bail and loan approval), it becomes important to explain how the system arrived at its decision, and also suggest actions to achieve a favorable decision. Counterfactual explanations -"how the world would have (had) to be different for a desirable outcome to occur"- aim to satisfy these criteria. Existing works have primarily focused on designing algorithms to obtain counterfactual explanations for a wide range of settings. However, it has largely been overlooked that ultimately, one of the main objectives is to allow people to act rather than just understand. In layman's terms, counterfactual explanations inform an individual where they need to get to, but not how to get there. In this work, we rely on causal reasoning to caution against the use of counterfactual explanations as a recommendable set of actions for recourse. Instead, we propose a shift of paradigm from recourse via nearest counterfactual explanations to recourse through minimal interventions, shifting the focus from explanations to interventions.},
booktitle = {Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages = {353–362},
numpages = {10},
keywords = {counterfactual explanations, algorithmic recourse, contrastive explanations, minimal interventions, consequential recommendations, explainable artificial intelligence, causal inference},
location = {Virtual Event, Canada},
series = {FAccT '21}
}



@InProceedings{Karimi.2019fy,
  title = 	 {Model-Agnostic Counterfactual Explanations for Consequential Decisions},
  author =       {Karimi, Amir-Hossein and Barthe, Gilles and Balle, Borja and Valera, Isabel},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {895--905},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v108/karimi20a/karimi20a.pdf},
  url = 	 {https://proceedings.mlr.press/v108/karimi20a.html},
  abstract = 	 {Predictive models are being increasingly used to support consequential decision making at the individual level in contexts such as pretrial bail and loan approval. As a result, there is increasing social and legal pressure to provide explanations that help the affected individuals not only to understand why a prediction was output, but also how to act to obtain a desired outcome. To this end, several works have proposed optimization-based methods to generate nearest counterfactual explanations. However, these methods are often restricted to a particular subset of models (e.g., decision trees or linear models) and differentiable distance functions. In contrast, we build on standard theory and tools from formal verification and propose a novel algorithm that solves a sequence of satisfiability problems, where both the distance function (objective) and predictive model (constraints) are represented as logic formulae. As shown by our experiments on real-world data, our algorithm is: i) model-agnostic ({non-}linear, {non-}differentiable, {non-}convex); ii) data-type-agnostic (heterogeneous features); iii) distance-agnostic (l0, l1, l8, and combinations thereof); iv) able to generate plausible and diverse counterfactuals for any sample (i.e., 100% coverage); and v) at provably optimal distances.}
}

@inproceedings{Mothilal.2020,
author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
title = {Explaining machine learning classifiers through diverse counterfactual explanations},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372850},
doi = {10.1145/3351095.3372850},
abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {607–617},
numpages = {11},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{Mahajan.2019, 
year = {2019}, 
keywords = {counterfactuals}, 
title = {{Preserving causal constraints in counterfactual explanations for machine learning classifiers}},
numpages={9},
author = {Mahajan, Divyat and Tan, Chenhao and Sharma, Amit}, 
journal = {arXiv preprint arXiv:1912.03277},
eprint = {1912.03277}, 
url={https://arxiv.org/abs/1912.03277},
abstract = {{To construct interpretable explanations that are consistent with the original ML model, counterfactual examples---showing how the model's output changes with small perturbations to the input---have been proposed. This paper extends the work in counterfactual explanations by addressing the challenge of feasibility of such examples. For explanations of ML models in critical domains such as healthcare and finance, counterfactual examples are useful for an end-user only to the extent that perturbation of feature inputs is feasible in the real world. We formulate the problem of feasibility as preserving causal relationships among input features and present a method that uses (partial) structural causal models to generate actionable counterfactuals. When feasibility constraints cannot be easily expressed, we consider an alternative mechanism where people can label generated CF examples on feasibility: whether it is feasible to intervene and realize the candidate CF example from the original input. To learn from this labelled feasibility data, we propose a modified variational auto encoder loss for generating CF examples that optimizes for feasibility as people interact with its output. Our experiments on Bayesian networks and the widely used ''Adult-Income'' dataset show that our proposed methods can generate counterfactual explanations that better satisfy feasibility constraints than existing methods.. Code repository can be accessed here: \textbackslashtextit\{https://github.com/divyat09/cf-feasibility\}}}
}


@inproceedings{
maragno2022counterfactual,
title={Counterfactual Explanations Using Optimization With Constraint Learning},
author={Donato Maragno and Tabea Elina R{\"o}ber and Ilker Birbil},
booktitle={OPT 2022: Optimization for Machine Learning (NeurIPS 2022 Workshop)},
year={2022},
url={https://openreview.net/forum?id=mcjDldTC3F2}
}


@inproceedings{byrne-ijcai2019p876-old,
	author = {Byrne, Ruth M. J.},
	booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	month = {7},
	pages = {6276--6282},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning},
	year = {2019}}


@inproceedings{byrne-ijcai2019p876,
	author = {Byrne, Ruth M. J.},
	booktitle = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, {IJCAI-19}},
	doi = {10.24963/ijcai.2019/876},
	month = {7},
	pages = {6276--6282},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	title = {Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning},
	url = {https://doi.org/10.24963/ijcai.2019/876},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.24963/ijcai.2019/876}}


@article{MILLER20191,
	abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
	author = {Tim Miller},
	doi = {https://doi.org/10.1016/j.artint.2018.07.007},
	issn = {0004-3702},
	journal = {Artificial Intelligence},
	keywords = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
	pages = {1-38},
	title = {Explanation in artificial intelligence: Insights from the social sciences},
	url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	volume = {267},
	year = {2019},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
	bdsk-url-2 = {https://doi.org/10.1016/j.artint.2018.07.007}}

@InProceedings{Selvaraju_2017_ICCV,
author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
title = {Grad-CAM: Visual Explanations From Deep Networks via Gradient-Based Localization},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017}
}

@misc{IMDRF-2013,
    author = {{International Medical Device Regulators Forum (IMDRF)}},
    year = {2013},
    url = {https://www.imdrf.org/sites/default/files/docs/imdrf/final/technical/imdrf-tech-131209-samd-key-definitions-140901.pdf},
title = {{Software as a Medical Device (SaMD): Key Definitions}},
note = {Accessed: 2025-01-27}
}



@article{deHond-2023,
	author = {de Hond, Anne A. H. and Kant, Ilse M. J. and Fornasa, Mattia and Cin{\`a}, Giovanni and Elbers, Paul W. G. and Thoral, Patrick J. and Sesmu Arbous, M. and Steyerberg, Ewout W.},
	date-added = {2024-08-27 09:57:56 +0200},
	date-modified = {2024-08-27 09:57:56 +0200},
	id = {00003246-202302000-00011},
	isbn = {0090-3493},
	journal = {Critical Care Medicine},
	keywords = {clinical decision support; critical care; data science; external validation; generalizability; machine learning},
	n2 = {OBJECTIVES:  Many machine learning (ML) models have been developed for application in the ICU, but few models have been subjected to external validation. The performance of these models in new settings therefore remains unknown. The objective of this study was to assess the performance of an existing decision support tool based on a ML model predicting readmission or death within 7 days after ICU discharge before, during, and after retraining and recalibration.  DESIGN:  A gradient boosted ML model was developed and validated on electronic health record data from 2004 to 2021. We performed an independent validation of this model on electronic health record data from 2011 to 2019 from a different tertiary care center.  SETTING:  Two ICUs in tertiary care centers in The Netherlands.  PATIENTS:  Adult patients who were admitted to the ICU and stayed for longer than 12 hours.  INTERVENTIONS:  None.  MEASUREMENTS AND MAIN RESULTS:  We assessed discrimination by area under the receiver operating characteristic curve (AUC) and calibration (slope and intercept). We retrained and recalibrated the original model and assessed performance via a temporal validation design. The final retrained model was cross-validated on all data from the new site. Readmission or death within 7 days after ICU discharge occurred in 577 of 10,052 ICU admissions (5.7%) at the new site. External validation revealed moderate discrimination with an AUC of 0.72 (95% CI 0.67--0.76). Retrained models showed improved discrimination with AUC 0.79 (95% CI 0.75--0.82) for the final validation model. Calibration was poor initially and good after recalibration via isotonic regression.  CONCLUSIONS:  In this era of expanding availability of ML models, external validation and retraining are key steps to consider before applying ML models to new settings. Clinicians and decision-makers should take this into account when considering applying new ML models to their local settings.},
	number = {2},
	title = {Predicting Readmission or Death After Discharge From the ICU: External Validation and Retraining of a Machine Learning Model},
	url = {https://journals.lww.com/ccmjournal/fulltext/2023/02000/predicting_readmission_or_death_after_discharge.11.aspx},
	volume = {51},
	year = {2023},
	bdsk-url-1 = {https://journals.lww.com/ccmjournal/fulltext/2023/02000/predicting_readmission_or_death_after_discharge.11.aspx}}



@article{Koh2022-review,
	abstract = {An increasing array of tools is being developed using artificial intelligence (AI) and machine learning (ML) for cancer imaging. The development of an optimal tool requires multidisciplinary engagement to ensure that the appropriate use case is met, as well as to undertake robust development and testing prior to its adoption into healthcare systems. This multidisciplinary review highlights key developments in the field. We discuss the challenges and opportunities of {AI} and {ML} in cancer imaging; considerations for the development of algorithms into tools that can be widely used and disseminated; and the development of the ecosystem needed to promote growth of {AI} and {ML} in cancer imaging.},
	author = {Koh, Dow-Mu and Papanikolaou, Nickolas and Bick, Ulrich and Illing, Rowland and Kahn, Charles E. and Kalpathi-Cramer, Jayshree and Matos, Celso and Mart{\'\i}-Bonmat{\'\i}, Luis and Miles, Anne and Mun, Seong Ki and Napel, Sandy and Rockall, Andrea and Sala, Evis and Strickland, Nicola and Prior, Fred},
	date = {2022/10/27},
	date-added = {2024-08-27 10:51:52 +0200},
	date-modified = {2024-08-27 10:51:52 +0200},
	doi = {10.1038/s43856-022-00199-0},
	id = {Koh2022},
	isbn = {2730-664X},
	journal = {Communications Medicine},
	number = {1},
	pages = {133},
	title = {Artificial intelligence and machine learning in cancer imaging},
	url = {https://doi.org/10.1038/s43856-022-00199-0},
	volume = {2},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1038/s43856-022-00199-0}}

@article{miller1956magical,
  title={The magical number seven, plus or minus two: Some limits on our capacity for processing information.},
  author={Miller, George A},
  journal={Psychological Review},
  volume={63},
  number={2},
  pages={81},
  year={1956},
  publisher={American Psychological Association}
}

@article{Cowan_2001, title={The magical number 4 in short-term memory: A reconsideration of mental storage capacity}, volume={24}, DOI={10.1017/S0140525X01003922}, number={1}, journal={Behavioral and Brain Sciences}, author={Cowan, Nelson}, year={2001}, pages={87–114}, url={https://doi.org/10.1017/S0140525X01003922}} 


@article{Luck1997,
	abstract = {Short-term memory storage can be divided into separate subsystems for verbal information and visual information1, and recent studies have begun to delineate the neural substrates of these working-memory systems2,3,4,5,6. Although the verbal storage system has been well characterized, the storage capacity of visual working memory has not yet been established for simple, suprathreshold features or for conjunctions of features. Here we demonstrate that it is possible to retain information about only four colours or orientations in visual working memory at one time. However, it is also possible to retain both the colour and the orientation of four objects, indicating that visual working memory stores integrated objects rather than individual features. Indeed, objects defined by a conjunction of four features can be retained in working memory just as well as single-feature objects, allowing sixteen individual features to be retained when distributed across four objects. Thus, the capacity of visual working memory must be understood in terms of integrated objects rather than individual features, which places significant constraints on cognitive and neurobiological models of the temporary storage of visual information7.},
	author = {Luck, Steven J. and Vogel, Edward K.},
	date = {1997/11/01},
	date-added = {2024-08-28 16:32:53 +0200},
	date-modified = {2024-08-28 16:32:53 +0200},
	doi = {10.1038/36846},
	id = {Luck1997},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6657},
	pages = {279--281},
	title = {The capacity of visual working memory for features and conjunctions},
	url = {https://doi.org/10.1038/36846},
	volume = {390},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1038/36846}}


@article{OBERAUER2006601,
	abstract = {A mathematical model of working-memory capacity limits is proposed on the key assumption of mutual interference between items in working memory. Interference is assumed to arise from overwriting of features shared by these items. The model was fit to time-accuracy data of memory-updating tasks from four experiments using nonlinear mixed effect (NLME) models as a framework. The model gave a good account of the data from a numerical and a spatial task version. The performance pattern in a combination of numerical and spatial updating could be explained by variations in the interference parameter: assuming less feature overlap between contents from different domains than between contents from the same domain, the model can account for double dissociations of content domains in dual-task experiments. Experiment 3 extended this idea to similarity within the verbal domain. The decline of memory accuracy with increasing memory load was steeper with phonologically similar than with dissimilar material, although processing speed was faster for the similar material. The model captured the similarity effects with a higher estimated interference parameter for the similar than for the dissimilar condition. The results are difficult to explain with alternative models, in particular models incorporating time-based decay and models assuming limited resource pools.},
	author = {Klaus Oberauer and Reinhold Kliegl},
	doi = {https://doi.org/10.1016/j.jml.2006.08.009},
	issn = {0749-596X},
	journal = {Journal of Memory and Language},
	keywords = {Working memory, Interference, Capacity, Mathematical model, Non-linear mixed effects},
	note = {Special Issue on Memory Models},
	number = {4},
	pages = {601-626},
	title = {A formal model of capacity limits in working memory},
	url = {https://www.sciencedirect.com/science/article/pii/S0749596X06000982},
	volume = {55},
	year = {2006},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0749596X06000982},
	bdsk-url-2 = {https://doi.org/10.1016/j.jml.2006.08.009}}

@article{broder2003take,
  title={Take The Best versus simultaneous feature matching: Probabilistic inferences from memory and effects of reprensentation format},
  author={Br{\"o}der, Arndt and Schiffer, Stefanie},
  journal={Journal of Experimental Psychology: General},
  volume={132},
  number={2},
  pages={277},
  year={2003},
  publisher={American Psychological Association},
  url={https://psycnet.apa.org/doi/10.1037/0096-3445.132.2.277},
  doi = {10.1037/0096-3445.132.2.277}
}

@article{shah2008heuristics,
  title={Heuristics made easy: an effort-reduction framework},
  author={Shah, Anuj K and Oppenheimer, Daniel M},
  journal={Psychological Bulletin},
  volume={134},
  number={2},
  pages={207--222},
  year={2008},
  publisher={American Psychological Association},
url={https://psycnet.apa.org/doi/10.1037/0033-2909.134.2.207},
doi={10.1037/0033-2909.134.2.207}
}


@misc{bbc-2018,
    author = {BBC},
    year = {2018},
    url = {https://www.bbc.com/news/technology-45809919},
    title = {Amazon scrapped `sexist {AI}' tool}
}


@article{Thoral2021,
	author = {Thoral, Patrick J. and Fornasa, Mattia and de Bruin, Daan P. and Tonutti, Michele and Hovenkamp, Hidde and Driessen, Ronald H. and Girbes, Armand R. J. and Hoogendoorn, Mark and Elbers, Paul W. G.},
	date-added = {2024-08-30 13:12:34 +0200},
	date-modified = {2024-08-30 13:12:34 +0200},
	id = {02107256-202109000-00013},
	journal = {Critical Care Explorations},
	keywords = {decision support techniques; information dissemination; machine learning; mortality; patient discharge; patient readmission},
	n2 = {Objectives:  Unexpected ICU readmission is associated with longer length of stay and increased mortality. To prevent ICU readmission and death after ICU discharge, our team of intensivists and data scientists aimed to use AmsterdamUMCdb to develop an explainable machine learning--based real-time bedside decision support tool.  Derivation Cohort:  Data from patients admitted to a mixed surgical-medical academic medical center ICU from 2004 to 2016.  Validation Cohort:  Data from 2016 to 2019 from the same center.  Prediction Model:  Patient characteristics, clinical observations, physiologic measurements, laboratory studies, and treatment data were considered as model features. Different supervised learning algorithms were trained to predict ICU readmission and/or death, both within 7 days from ICU discharge, using 10-fold cross-validation. Feature importance was determined using SHapley Additive exPlanations, and readmission probability-time curves were constructed to identify subgroups. Explainability was established by presenting individualized risk trends and feature importance.  Results:  Our final derivation dataset included 14,105 admissions. The combined readmission/mortality rate within 7 days of ICU discharge was 5.3%. Using Gradient Boosting, the model achieved an area under the receiver operating characteristic curve of 0.78 (95% CI, 0.75--0.81) and an area under the precision-recall curve of 0.19 on the validation cohort (n = 3,929). The most predictive features included common physiologic parameters but also less apparent variables like nutritional support. At a 6% risk threshold, the model showed a sensitivity (recall) of 0.72, specificity of 0.70, and a positive predictive value (precision) of 0.15. Impact analysis using probability-time curves and the 6% risk threshold identified specific patient groups at risk and the potential of a change in discharge management to reduce relative risk by 14%.  Conclusions:  We developed an explainable machine learning model that may aid in identifying patients at high risk for readmission and mortality after ICU discharge using the first freely available European critical care database, AmsterdamUMCdb. Impact analysis showed that a relative risk reduction of 14% could be achievable, which might have significant impact on patients and society. ICU data sharing facilitates collaboration between intensivists and data scientists to accelerate model development.},
	number = {9},
	title = {Explainable Machine Learning on AmsterdamUMCdb for ICU Discharge Decision Support: Uniting Intensivists and Data Scientists},
	url = {https://journals.lww.com/ccejournal/fulltext/2021/09000/explainable_machine_learning_on_amsterdamumcdb_for.13.aspx},
	volume = {3},
	year = {2021},
	bdsk-url-1 = {https://journals.lww.com/ccejournal/fulltext/2021/09000/explainable_machine_learning_on_amsterdamumcdb_for.13.aspx}}


@article{Cai2019,
author = {Cai, Carrie J. and Winter, Samantha and Steiner, David and Wilcox, Lauren and Terry, Michael},
title = {"Hello {AI}": Uncovering the Onboarding Needs of Medical Practitioners for Human-{AI} Collaborative Decision-Making},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {CSCW},
url = {https://doi.org/10.1145/3359206},
doi = {10.1145/3359206},
abstract = {Although rapid advances in machine learning have made it increasingly applicable to expert decision-making, the delivery of accurate algorithmic predictions alone is insufficient for effective human-AI collaboration. In this work, we investigate the key types of information medical experts desire when they are first introduced to a diagnostic AI assistant. In a qualitative lab study, we interviewed 21 pathologists before, during, and after being presented deep neural network (DNN) predictions for prostate cancer diagnosis, to learn the types of information that they desired about the AI assistant. Our findings reveal that, far beyond understanding the local, case-specific reasoning behind any model decision, clinicians desired upfront information about basic, global properties of the model, such as its known strengths and limitations, its subjective point-of-view, and its overall design objective--what it's designed to be optimized for. Participants compared these information needs to the collaborative mental models they develop of their medical colleagues when seeking a second opinion: the medical perspectives and standards that those colleagues embody, and the compatibility of those perspectives with their own diagnostic patterns. These findings broaden and enrich discussions surrounding AI transparency for collaborative decision-making, providing a richer understanding of what experts find important in their introduction to AI assistants before integrating them into routine practice.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {nov},
articleno = {104},
numpages = {24},
keywords = {clinical health, human-ai interaction, machine learning}
}

@ARTICLE{El-Sappagh2018,
  author={El-Sappagh, Shaker and Alonso, José M. and Ali, Farman and Ali, Amjad and Jang, Jun-Hyeog and Kwak, Kyung-Sup},
  journal={IEEE Access}, 
  title={An Ontology-Based Interpretable Fuzzy Decision Support System for Diabetes Diagnosis}, 
  year={2018},
  volume={6},
  number={},
  pages={37371-37394},
  keywords={Diabetes;Cognition;Ontologies;Medical diagnostic imaging;Diseases;Semantics;Clinical decision support system;diabetes diagnosis;fuzzy inference system;ontology reasoning;fuzzy interpretability},
  doi={10.1109/ACCESS.2018.2852004}}



@InProceedings{pmlr-v106-tonekaboni19a,
  title = 	 {What Clinicians Want: Contextualizing Explainable Machine Learning for Clinical End Use},
  author =       {Tonekaboni, Sana and Joshi, Shalmali and McCradden, Melissa D. and Goldenberg, Anna},
  booktitle = 	 {Proceedings of the 4th Machine Learning for Healthcare Conference},
  pages = 	 {359--380},
  year = 	 {2019},
  editor = 	 {Doshi-Velez, Finale and Fackler, Jim and Jung, Ken and Kale, David and Ranganath, Rajesh and Wallace, Byron and Wiens, Jenna},
  volume = 	 {106},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--10 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v106/tonekaboni19a/tonekaboni19a.pdf},
  url = 	 {https://proceedings.mlr.press/v106/tonekaboni19a.html},
  abstract = 	 {Translating machine learning (ML) models effectively to clinical practice requires establishing clinicians’ trust. Explainability, or the ability of an ML model to justify its outcomes and assist clinicians in rationalizing the model prediction, has been generally understood to be critical to establishing trust. However, the eld suffers from the lack of concrete definitions for usable explanations in different settings. To identify specific aspects of explainability that may catalyze building trust in ML models, we surveyed clinicians from two distinct acute care specialties (Intenstive Care Unit and Emergency Department). We use their feedback to characterize when explainability helps to improve clinicians’ trust in ML models. We further identify the classes of explanations that clinicians identified as most relevant and crucial for effective translation to clinical practice. Finally, we discern concrete metrics for rigorous evaluation of clinical explainability methods. By integrating perceptions of explainability between clinicians and ML researchers we hope to facilitate the endorsement and broader adoption and sustained use of ML systems in healthcare.}
}

@article{lee2024improvinghealthprofessionalsonboarding,
      title={Improving Health Professionals' Onboarding with {AI} and {XAI} for Trustworthy Human-{AI} Collaborative Decision Making}, 
      author={Min Hun Lee and Silvana Xin Yi Choo and Shamala D/O Thilarajah},
      year={2024},
      eprint={2405.16424},
      archivePrefix={arXiv},
      primaryClass={cs.HC},
      url={https://arxiv.org/abs/2405.16424},
journal = {arXiv preprint arXiv:2405.16424}
}


@article{Bienefeld2023,
	abstract = {Explainable artificial intelligence (XAI) has emerged as a promising solution for addressing the implementation challenges of AI/ML in healthcare. However, little is known about how developers and clinicians interpret XAI and what conflicting goals and requirements they may have. This paper presents the findings of a longitudinal multi-method study involving 112 developers and clinicians co-designing an XAI solution for a clinical decision support system. Our study identifies three key differences between developer and clinician mental models of XAI, including opposing goals (model interpretability vs. clinical plausibility), different sources of truth (data vs. patient), and the role of exploring new vs. exploiting old knowledge. Based on our findings, we propose design solutions that can help address the XAI conundrum in healthcare, including the use of causal inference models, personalized explanations, and ambidexterity between exploration and exploitation mindsets. Our study highlights the importance of considering the perspectives of both developers and clinicians in the design of XAI systems and provides practical recommendations for improving the effectiveness and usability of XAI in healthcare.},
	author = {Bienefeld, Nadine and Boss, Jens Michael and L{\"u}thy, Rahel and Brodbeck, Dominique and Azzati, Jan and Blaser, Mirco and Willms, Jan and Keller, Emanuela},
	date = {2023/05/22},
	date-added = {2024-08-30 15:27:13 +0200},
	date-modified = {2024-08-30 15:27:13 +0200},
	doi = {10.1038/s41746-023-00837-4},
	id = {Bienefeld2023},
	isbn = {2398-6352},
	journal = {npj Digital Medicine},
	number = {1},
	pages = {94},
	title = {Solving the explainable {AI} conundrum by bridging clinicians'needs and developers'goals},
	url = {https://doi.org/10.1038/s41746-023-00837-4},
	volume = {6},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1038/s41746-023-00837-4}}


@inproceedings{Sendak2020,
author = {Sendak, Mark and Elish, Madeleine Clare and Gao, Michael and Futoma, Joseph and Ratliff, William and Nichols, Marshall and Bedoya, Armando and Balu, Suresh and O'Brien, Cara},
title = {"The human body is a black box": supporting clinical decision-making with deep learning},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372827},
doi = {10.1145/3351095.3372827},
abstract = {Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {99–109},
numpages = {11},
keywords = {trust, medicine, interpretability, expertise, deep learning},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@inproceedings{Yang2019-unremarkableness,
author = {Yang, Qian and Steinfeld, Aaron and Zimmerman, John},
title = {Unremarkable AI: Fitting Intelligent Decision Support into Critical, Clinical Decision-Making Processes},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300468},
doi = {10.1145/3290605.3300468},
abstract = {Clinical decision support tools (DST) promise improved healthcare outcomes by offering data-driven insights. While effective in lab settings, almost all DSTs have failed in practice. Empirical research diagnosed poor contextual fit as the cause. This paper describes the design and field evaluation of a radically new form of DST. It automatically generates slides for clinicians' decision meetings with subtly embedded machine prognostics. This design took inspiration from the notion of Unremarkable Computing, that by augmenting the users' routines technology/AI can have significant importance for the users yet remain unobtrusive. Our field evaluation suggests clinicians are more likely to encounter and embrace such a DST. Drawing on their responses, we discuss the importance and intricacies of finding the right level of unremarkableness in DST design, and share lessons learned in prototyping critical AI systems as a situated experience.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {decision support systems, healthcare, user experience},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@book{rogers2003diffusion,
  title={Diffusion of Innovations},
  author={Rogers, Everett M.},
  edition={5th},
  year={2003},
  publisher={Free Press},
  address={New York},
  isbn={978-0743222099}
}


@article{van2022developing,
  title={Developing, implementing and governing artificial intelligence in medicine: a step-by-step approach to prevent an artificial intelligence winter},
  author={Van de Sande, Davy and Van Genderen, Michel E and Smit, Jim M and Huiskens, Joost and Visser, Jacob J and Veen, Robert ER and van Unen, Edwin and Hilgers, Oliver and Gommers, Diederik and van Bommel, Jasper},
  journal={BMJ Health \& Care Informatics},
  volume={29},
  number={1},
  year={2022},
  publisher={BMJ Publishing Group},
url={https://doi.org/10.1136/bmjhci-2021-100495},
doi={10.1136/bmjhci-2021-100495}
}

@inproceedings{Semenova2022_10.1145/3531146.3533232,
author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
title = {On the Existence of Simpler Machine Learning Models},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3533232},
doi = {10.1145/3531146.3533232},
abstract = {It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1827–1858},
numpages = {32},
keywords = {Generalization, Interpretable Machine Learning, Model Multiplicity, Rashomon Set, Simplicity},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}


@article{NAISEH2023102941,
	abstract = {Machine learning has made rapid advances in safety-critical applications, such as traffic control, finance, and healthcare. With the criticality of decisions they support and the potential consequences of following their recommendations, it also became critical to provide users with explanations to interpret machine learning models in general, and black-box models in particular. However, despite the agreement on explainability as a necessity, there is little evidence on how recent advances in eXplainable Artificial Intelligence literature (XAI) can be applied in collaborative decision-making tasks, i.e., human decision-maker and an AI system working together, to contribute to the process of trust calibration effectively. This research conducts an empirical study to evaluate four XAI classes for their impact on trust calibration. We take clinical decision support systems as a case study and adopt a within-subject design followed by semi-structured interviews. We gave participants clinical scenarios and XAI interfaces as a basis for decision-making and rating tasks. Our study involved 41 medical practitioners who use clinical decision support systems frequently. We found that users perceive the contribution of explanations to trust calibration differently according to the XAI class and to whether XAI interface design fits their job constraints and scope. We revealed additional requirements on how explanations shall be instantiated and designed to help a better trust calibration. Finally, we build on our findings and present guidelines for designing XAI interfaces.},
	author = {Mohammad Naiseh and Dena Al-Thani and Nan Jiang and Raian Ali},
	doi = {https://doi.org/10.1016/j.ijhcs.2022.102941},
	issn = {1071-5819},
	journal = {International Journal of Human-Computer Studies},
	keywords = {Explainable AI, Clinical decision support systems, Human-AI Interaction, Trust Calibration},
	pages = {102941},
	title = {How the different explanation classes impact trust calibration: The case of clinical decision support systems},
	url = {https://www.sciencedirect.com/science/article/pii/S1071581922001616},
	volume = {169},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S1071581922001616},
	bdsk-url-2 = {https://doi.org/10.1016/j.ijhcs.2022.102941}}

  

@article{SUBRAMANIAN2024102780,
	abstract = {The rise of complex AI systems in healthcare and other sectors has led to a growing area of research called Explainable AI (XAI) designed to increase transparency. In this area, quantitative and qualitative studies focus on improving user trust and task performance by providing system- and prediction-level XAI features. We analyze stakeholder engagement events (interviews and workshops) on the use of AI for kidney transplantation. From this we identify themes which we use to frame a scoping literature review on current XAI features. The stakeholder engagement process lasted over nine months covering three stakeholder group's workflows, determining where AI could intervene and assessing a mock XAI decision support system. Based on the stakeholder engagement, we identify four major themes relevant to designing XAI systems -- 1) use of AI predictions, 2) information included in AI predictions, 3) personalization of AI predictions for individual differences, and 4) customizing AI predictions for specific cases. Using these themes, our scoping literature review finds that providing AI predictions before, during, or after decision-making could be beneficial depending on the complexity of the stakeholder's task. Additionally, expert stakeholders like surgeons prefer minimal to no XAI features, AI prediction, and uncertainty estimates for easy use cases. However, almost all stakeholders prefer to have optional XAI features to review when needed, especially in hard-to-predict cases. The literature also suggests that providing both system- and prediction-level information is necessary to build the user's mental model of the system appropriately. Although XAI features improve users' trust in the system, human-AI team performance is not always enhanced. Overall, stakeholders prefer to have agency over the XAI interface to control the level of information based on their needs and task complexity. We conclude with suggestions for future research, especially on customizing XAI features based on preferences and tasks.},
	author = {Harishankar V. Subramanian and Casey Canfield and Daniel B. Shank},
	doi = {https://doi.org/10.1016/j.artmed.2024.102780},
	issn = {0933-3657},
	journal = {Artificial Intelligence in Medicine},
	keywords = {Explainable AI, Stakeholder engagement, Human-centered design, Human-AI team, Trust in AI, Kidney transplant},
	pages = {102780},
	title = {Designing explainable {AI} to improve human-{AI} team performance: A medical stakeholder-driven scoping review},
	url = {https://www.sciencedirect.com/science/article/pii/S0933365724000228},
	volume = {149},
	year = {2024},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0933365724000228},
	bdsk-url-2 = {https://doi.org/10.1016/j.artmed.2024.102780}}


@article{alshenqeeti2014interviewing,
  title={Interviewing as a data collection method: A critical review},
  author={Alshenqeeti, Hamza},
  journal={English Linguistics Research},
  volume={3},
  number={1},
  pages={39--45},
  year={2014},
  doi={10.5430/elr.v3n1p39},
  URL={http://dx.doi.org/10.5430/elr.v3n1p39}
}


@article{rober2024rulegenerationclassificationscalability,
      title={Rule Generation for Classification: Scalability, Interpretability, and Fairness}, 
      author={Tabea E. Röber and Adia C. Lumadjeng and M. Hakan Akyüz and Ş. İlker Birbil},
      year={2024},
      eprint={2104.10751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2104.10751},
    journal = {arXiv preprint arXiv:2104.10751},
eprint = {2104.10751}}


@article{Yang2023-familiarity,
	abstract = {The current study aims to explore one factor that likely contributes to these statistical regularities, familiarity. Are highly familiar stimuli perceived more readily? Previous work showing effects of familiarity on perception have used recognition tasks, which arguably tap into post-perceptual processes. Here we use a perceptual task that does not depend on explicit recognition; participants were asked to discriminate whether a rapidly presented image was intact or scrambled. The familiarity level of stimuli was manipulated. Results show that famous or upright orientated logos (Experiments 1 and 2) or faces (Experiment 3) were better discriminated than novel or inverted logos and faces. To further dissociate our task from recognition, we implemented a simple detection task (Experiment 4) and directly compared the intact/scrambled task to a recognition task (Experiment 5) on the same set of faces used in Experiment 3. The fame and orientation familiarity effect were still present in the simple detection task, and the duration needed on the intact/scrambled task was significantly less than the recognition task. We conclude that familiarity effect demonstrated here is not driven by explicit recognition and instead reflects a true perceptual effect.},
	author = {Yang, Pei-Ling and Beck, Diane M.},
	date = {2023/05/01},
	date-added = {2024-09-11 20:44:37 +0200},
	date-modified = {2024-09-11 20:44:37 +0200},
	doi = {10.3758/s13414-023-02703-7},
	id = {Yang2023},
	isbn = {1943-393X},
	journal = {Attention, Perception, \& Psychophysics},
	number = {4},
	pages = {1127--1149},
	title = {Familiarity influences visual detection in a task that does not require explicit recognition},
	url = {https://doi.org/10.3758/s13414-023-02703-7},
	volume = {85},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.3758/s13414-023-02703-7}}



@article{Braun01012006,
	author = {Virginia Braun and Victoria Clarke},
	doi = {10.1191/1478088706qp063oa},
	eprint = {https://www.tandfonline.com/doi/pdf/10.1191/1478088706qp063oa},
	journal = {Qualitative Research in Psychology},
	number = {2},
	pages = {77--101},
	publisher = {Routledge},
	title = {Using thematic analysis in psychology},
	url = {https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
	volume = {3},
	year = {2006},
	bdsk-url-1 = {https://www.tandfonline.com/doi/abs/10.1191/1478088706qp063oa},
	bdsk-url-2 = {https://doi.org/10.1191/1478088706qp063oa}}

@article{rudin_stop2019, 
year = {2019}, 
title = {{Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead}}, 
author = {Rudin, Cynthia}, 
journal = {Nature Machine Intelligence}, 
url = {https://doi.org/10.1038/s42256-019-0048-x}, 
abstract = {{Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision. There has been a recent rise of interest in developing methods for ‘explainable AI’, where models are created to explain how a first ‘black box’ machine learning model arrives at a specific decision. It can be argued that instead efforts should be directed at building inherently interpretable models in the first place, in particular where they are applied in applications that directly affect human lives, such as in healthcare and criminal justice.}}, 
pages = {206--215}, 
number = {5}, 
volume = {1}, 
}

@article{Ghassemi.2021, 
year = {2021}, 
keywords = {healthcare}, 
title = {{The false hope of current approaches to explainable artificial intelligence in health care}}, 
author = {Ghassemi, Marzyeh and Oakden-Rayner, Luke and Beam, Andrew L}, 
journal = {The Lancet Digital Health}, 
issn = {2589-7500}, 
url = {https://doi.org/10.1016/s2589-7500(21)00208-9},
pmid = {34711379}, 
abstract = {{The black-box nature of current artificial intelligence (AI) has caused some to question whether AI must be explainable to be used in high-stakes scenarios such as medicine. It has been argued that explainable AI will engender trust with the health-care workforce, provide transparency into the AI decision making process, and potentially mitigate various kinds of bias. In this Viewpoint, we argue that this argument represents a false hope for explainable AI and that current explainability methods are unlikely to achieve these goals for patient-level decision support. We provide an overview of current explainability techniques and highlight how various failure cases can cause problems for decision making for individual patients. In the absence of suitable explainability methods, we advocate for rigorous internal and external validation of AI models as a more direct means of achieving the goals often associated with explainability, and we caution against having explainability be a requirement for clinically deployed models.}}, 
pages = {e745--e750}, 
number = {11}, 
volume = {3}
}

@inproceedings{cina2023semanticmatch-old,
  title={Semantic match: Debugging feature attribution methods in {XAI} for healthcare},
  author={Cin\`a, Giovanni and R\"ober, Tabea E. and Goedhard, Rob and Birbil, {\c S} {\. I}lker},
  booktitle={Conference on Health, Inference, and Learning},
  pages={182--190},
  year={2023},
  organization={PMLR}
}


@InProceedings{cina2023semanticmatch,
  title = 	 {Semantic match: Debugging feature attribution methods \titlebreak in {XAI} for healthcare},
  author =       {Cina, Giovanni and Rober, Tabea E. and Goedhard, Rob and Birbil, S Ilker},
  booktitle = 	 {Proceedings of the Conference on Health, Inference, and Learning},
  pages = 	 {182--190},
  year = 	 {2023},
  editor = 	 {Mortazavi, Bobak J. and Sarker, Tasmie and Beam, Andrew and Ho, Joyce C.},
  volume = 	 {209},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {22 Jun--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v209/cina23a/cina23a.pdf},
  url = 	 {https://proceedings.mlr.press/v209/cina23a.html},
  abstract = 	 {The recent spike in certified Artificial Intelligence tools for healthcare has renewed the debate around adoption of this technology. One thread of such debate concerns Explainable AI and its promise to render AI devices more transparent and trustworthy. A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques and especially feature attribution methods, questioning their use and inclusion in guidelines and standards. We characterize the problem as a lack of semantic match between explanations and human understanding. To understand when feature importance can be used reliably, we introduce a distinction between feature importance of low- and high-level features. We argue that for data types where low-level features come endowed with a clear semantics, such as tabular data like Electronic Health Records, semantic match can be obtained, and thus feature attribution methods can still be employed in a meaningful and useful way. For high-level features, we sketch a procedure to test whether semantic match has been achieved.}
}


@inproceedings{wan2022bias,
author = {Wan, Charles and Belo, Rodrigo and Zejnilovic, Leid},
title = {{Explainability's gain is optimality's loss? How explanations bias decision-making}},
year = {2022},
isbn = {9781450392471},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514094.3534156},
abstract = {Decisions in organizations are about evaluating alternatives and choosing the one that would best serve organizational goals. To the extent that the evaluation of alternatives could be formulated as a predictive task with appropriate metrics, machine learning algorithms are increasingly being used to improve the efficiency of the process. Explanations help to facilitate communication between the algorithm and the human decision-maker, making it easier for the latter to interpret and make decisions on the basis of predictions by the former. Feature-based explanations' semantics of causal models, however, induce leakage from the decision-maker's prior beliefs. Our findings from a field experiment demonstrate empirically how this leads to confirmation bias and disparate impact on the decision-maker's confidence in the predictions. Such differences can lead to sub-optimal and biased decision outcomes.},
booktitle = {Proceedings of the 2022 AAAI/ACM Conference on AI, Ethics, and Society},
pages = {778–787},
numpages = {10},
keywords = {human-algorithm communication, semantics of explanations, bias, confirmation bias, decision-making, explanations, human causal reasoning},
location = {Oxford, United Kingdom},
series = {AIES '22}
}



@article{Bauer2023_XAI,
	abstract = { Although future regulations increasingly advocate that AI applications must be interpretable by users, we know little about how such explainability can affect human information processing. By conducting two experimental studies, we help to fill this gap. We show that explanations pave the way for AI systems to reshape users' understanding of the world around them. Specifically, state-of-the-art explainability methods evoke mental model adjustments that are subject to confirmation bias, allowing misconceptions and mental errors to persist and even accumulate. Moreover, mental model adjustments create spillover effects that alter users' behavior in related but distinct domains where they do not have access to an AI system. These spillover effects of mental model adjustments risk manipulating user behavior, promoting discriminatory biases, and biasing decision making. The reported findings serve as a warning that the indiscriminate use of modern explainability methods as an isolated measure to address AI systems' black-box problems can lead to unintended, unforeseen problems because it creates a new channel through which AI systems can influence human behavior in various domains. },
	author = {Bauer, Kevin and von Zahn, Moritz and Hinz, Oliver},
	doi = {10.1287/isre.2023.1199},
	eprint = {https://doi.org/10.1287/isre.2023.1199},
	journal = {Information Systems Research},
	number = {4},
	pages = {1582-1602},
	title = {Expl(AI)ned: The Impact of Explainable Artificial Intelligence on Users' Information Processing},
	url = {https://doi.org/10.1287/isre.2023.1199},
	volume = {34},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1287/isre.2023.1199}}


@article{Bauer2023_XAI-old,
author = {Bauer, Kevin and von Zahn, Moritz and Hinz, Oliver},
title = {Expl(AI)ned: The Impact of Explainable Artificial Intelligence on Users’ Information Processing},
journal = {Information Systems Research},
volume = {0},
number = {0},
year = {2023},
URL = { https://doi.org/10.1287/isre.2023.1199},
eprint = {https://doi.org/10.1287/isre.2023.1199},
    abstract = { Although future regulations increasingly advocate that AI applications must be interpretable by users, we know little about how such explainability can affect human information processing. By conducting two experimental studies, we help to fill this gap. We show that explanations pave the way for AI systems to reshape users' understanding of the world around them. Specifically, state-of-the-art explainability methods evoke mental model adjustments that are subject to confirmation bias, allowing misconceptions and mental errors to persist and even accumulate. Moreover, mental model adjustments create spillover effects that alter users' behavior in related but distinct domains where they do not have access to an AI system. These spillover effects of mental model adjustments risk manipulating user behavior, promoting discriminatory biases, and biasing decision making. The reported findings serve as a warning that the indiscriminate use of modern explainability methods as an isolated measure to address AI systems' black-box problems can lead to unintended, unforeseen problems because it creates a new channel through which AI systems can influence human behavior in various domains. }
}


@article{Eke2024,
	abstract = {The healthcare sector has advanced significantly as a result of the ability of artificial intelligence (AI) to solve cognitive problems that once required human intelligence. As artificial intelligence finds more applications in healthcare, trustworthiness must be guaranteed. Even while AI has the potential to improve healthcare, there are still challenging issues because it is yet to be widely adopted, especially when it comes to transparency. Concerns about comprehending the internal workings of AI models, possible biases, model robustness, and generalizability are raised by their opacity which makes them function like black boxes. A solution for worries over the transparency of AI algorithms is explainable AI. Explainable AI seeks to enhance AI explainability and analytical capabilities, particularly in vital industries like healthcare. Even though earlier research has examined several explainable AI-related topics, such as a lexicon, industry-specific overviews, and applications in the healthcare industry, a thorough analysis concentrating on the function of explainable AI in building trust in AI healthcare systems is required. In an effort to close this gap, a systematic literature review that adheres to PRISMA principles that analyze relevant papers that were published between 2015 and 2023 was done in this paper. To determine the critical role that explainable AI plays in fostering trust, this study examines widely utilized methodologies, machine learning and deep learning techniques, datasets, performance measures and validation procedures used in AI healthcare research. In addition, research issues and potential research directions are also discussed in this research. Thus, this systematic review provides a thorough summary of the present status of research on explainability and transparency in AI healthcare systems, thus illuminating crucial factors that affect user trust. The results are intended to assist researchers, policymakers and healthcare professionals in developing a more transparent, responsible and reliable AI system in the healthcare sector.},
	author = {Eke, Christopher Ifeanyi and Shuib, Liyana},
	date = {2024/12/17},
	date-added = {2025-01-10 16:07:06 +0100},
	date-modified = {2025-01-10 16:07:06 +0100},
	doi = {10.1007/s00521-024-10868-x},
	id = {Eke2024},
	isbn = {1433-3058},
	journal = {Neural Computing and Applications},
	title = {The role of explainability and transparency in fostering trust in {AI} healthcare systems: a systematic literature review, open issues and potential solutions},
	url = {https://doi.org/10.1007/s00521-024-10868-x},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s00521-024-10868-x}}



@article{He2019,
	abstract = {The development of artificial intelligence (AI)-based technologies in medicine is advancing rapidly, but real-world clinical implementation has not yet become a reality. Here we review some of the key practical issues surrounding the implementation of AI into existing clinical workflows, including data sharing and privacy, transparency of algorithms, data standardization, and interoperability across multiple platforms, and concern for patient safety. We summarize the current regulatory environment in the United States and highlight comparisons with other regions in the world, notably Europe and China.},
	author = {He, Jianxing and Baxter, Sally L. and Xu, Jie and Xu, Jiming and Zhou, Xingtao and Zhang, Kang},
	date = {2019/01/01},
	date-added = {2025-01-10 16:07:39 +0100},
	date-modified = {2025-01-10 16:07:39 +0100},
	doi = {10.1038/s41591-018-0307-0},
	id = {He2019},
	isbn = {1546-170X},
	journal = {Nature Medicine},
	number = {1},
	pages = {30--36},
	title = {The practical implementation of artificial intelligence technologies in medicine},
	url = {https://doi.org/10.1038/s41591-018-0307-0},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41591-018-0307-0}}


@article{Alowais2023,
	abstract = {Healthcare systems are complex and challenging for all stakeholders, but artificial intelligence (AI) has transformed various fields, including healthcare, with the potential to improve patient care and quality of life. Rapid AI advancements can revolutionize healthcare by integrating it into clinical practice. Reporting AI's role in clinical practice is crucial for successful implementation by equipping healthcare providers with essential knowledge and tools.},
	author = {Alowais, Shuroug A. and Alghamdi, Sahar S. and Alsuhebany, Nada and Alqahtani, Tariq and Alshaya, Abdulrahman I. and Almohareb, Sumaya N. and Aldairem, Atheer and Alrashed, Mohammed and Bin Saleh, Khalid and Badreldin, Hisham A. and Al Yami, Majed S. and Al Harbi, Shmeylan and Albekairy, Abdulkareem M.},
	date = {2023/09/22},
	date-added = {2025-01-10 16:12:08 +0100},
	date-modified = {2025-01-10 16:12:08 +0100},
	doi = {10.1186/s12909-023-04698-z},
	id = {Alowais2023},
	isbn = {1472-6920},
	journal = {BMC Medical Education},
	number = {1},
	pages = {689},
	title = {Revolutionizing healthcare: the role of artificial intelligence in clinical practice},
	url = {https://doi.org/10.1186/s12909-023-04698-z},
	volume = {23},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1186/s12909-023-04698-z}}



@article{Saarela2024,
	abstract = {This systematic literature review employs the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) methodology to investigate recent applications of explainable AI (XAI) over the past three years. From an initial pool of 664 articles identified through the Web of Science database, 512 peer-reviewed journal articles met the inclusion criteria---namely, being recent, high-quality XAI application articles published in English---and were analyzed in detail. Both qualitative and quantitative statistical techniques were used to analyze the identified articles: qualitatively by summarizing the characteristics of the included studies based on predefined codes, and quantitatively through statistical analysis of the data. These articles were categorized according to their application domains, techniques, and evaluation methods. Health-related applications were particularly prevalent, with a strong focus on cancer diagnosis, COVID-19 management, and medical imaging. Other significant areas of application included environmental and agricultural management, industrial optimization, cybersecurity, finance, transportation, and entertainment. Additionally, emerging applications in law, education, and social care highlight XAI's expanding impact. The review reveals a predominant use of local explanation methods, particularly SHAP and LIME, with SHAP being favored for its stability and mathematical guarantees. However, a critical gap in the evaluation of XAI results is identified, as most studies rely on anecdotal evidence or expert opinion rather than robust quantitative metrics. This underscores the urgent need for standardized evaluation frameworks to ensure the reliability and effectiveness of XAI applications. Future research should focus on developing comprehensive evaluation standards and improving the interpretability and stability of explanations. These advancements are essential for addressing the diverse demands of various application domains while ensuring trust and transparency in AI systems.},
	article-number = {8884},
	author = {Saarela, Mirka and Podgorelec, Vili},
	doi = {10.3390/app14198884},
	issn = {2076-3417},
	journal = {Applied Sciences},
	number = {19},
	title = {Recent Applications of Explainable {AI} ({XAI}): A Systematic Literature Review},
	url = {https://www.mdpi.com/2076-3417/14/19/8884},
	volume = {14},
	year = {2024},
	bdsk-url-1 = {https://www.mdpi.com/2076-3417/14/19/8884},
	bdsk-url-2 = {https://doi.org/10.3390/app14198884}}

@article{Nauta2023,
author = {Nauta, Meike and Trienes, Jan and Pathak, Shreyasi and Nguyen, Elisa and Peters, Michelle and Schmitt, Yasmin and Schl\"{o}tterer, J\"{o}rg and van Keulen, Maurice and Seifert, Christin},
title = {From Anecdotal Evidence to Quantitative Evaluation Methods: A Systematic Review on Evaluating Explainable AI},
year = {2023},
issue_date = {December 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {55},
number = {13s},
issn = {0360-0300},
url = {https://doi.org/10.1145/3583558},
doi = {10.1145/3583558},
abstract = {The rising popularity of explainable artificial intelligence (XAI) to understand high-performing black boxes raised the question of how to evaluate explanations of machine learning (ML) models. While interpretability and explainability are often presented as a subjectively validated binary property, we consider it a multi-faceted concept. We identify 12 conceptual properties, such as Compactness and Correctness, that should be evaluated for comprehensively assessing the quality of an explanation. Our so-called Co-12 properties serve as categorization scheme for systematically reviewing the evaluation practices of more than 300 papers published in the past 7 years at major AI and ML conferences that introduce an XAI method. We find that one in three papers evaluate exclusively with anecdotal evidence, and one in five papers evaluate with users. This survey also contributes to the call for objective, quantifiable evaluation methods by presenting an extensive overview of quantitative XAI evaluation methods. Our systematic collection of evaluation methods provides researchers and practitioners with concrete tools to thoroughly validate, benchmark, and compare new and existing XAI methods. The Co-12 categorization scheme and our identified evaluation methods open up opportunities to include quantitative metrics as optimization criteria during model training to optimize for accuracy and interpretability simultaneously.},
journal = {ACM Comput. Surv.},
month = jul,
articleno = {295},
numpages = {42},
keywords = {Explainable artificial intelligence, interpretable machine learning, evaluation, explainability, interpretability, quantitative evaluation methods, explainable AI, XAI}
}


@article{Borys2023,
	annote = {doi: 10.1016/j.ejrad.2023.110787},
	author = {Borys, Katarzyna and Schmitt, Yasmin Alyssa and Nauta, Meike and Seifert, Christin and Kr{\"a}mer, Nicole and Friedrich, Christoph M. and Nensa, Felix},
	date = {2023/05/01},
	date-added = {2025-01-10 17:07:41 +0100},
	date-modified = {2025-01-10 17:07:41 +0100},
	doi = {10.1016/j.ejrad.2023.110787},
	isbn = {0720-048X},
	journal = {European Journal of Radiology},
	journal1 = {European Journal of Radiology},
	month = {2025/01/10},
	publisher = {Elsevier},
    title = {Explainable {AI} in medical imaging: An overview for clinical practitioners – saliency-based {XAI} approaches},
	type = {doi: 10.1016/j.ejrad.2023.110787},
	url = {https://doi.org/10.1016/j.ejrad.2023.110787},
	volume = {162},
	year = {2023},
	year1 = {2023},
	bdsk-url-1 = {https://doi.org/10.1016/j.ejrad.2023.110787}}


@article{Gupta2024,
	abstract = {Artificial intelligence technologies such as machine learning and deep learning employ techniques to anticipate results more effectively without human involvement. Since AI models are viewed as opaque models, their application in healthcare is still restricted. Explainable artificial intelligence (XAI) has been designed to increase the use of artificial intelligence (AI) algorithms in the healthcare sector by increasing trust in the model's predictions and explaining how they are developed. The aim of this article is to critically review, compare, and summarize existing research and to find new research possibilities of XAI for applications in healthcare. This study is conducted by finding articles related to XAI in biological and healthcare domains from the PubMed, Science Direct, and Web of Science databases using the PRISMA method. A comparative study of the state-of-the-art XAI techniques to evaluate its applications in healthcare has also been done using an experimental demonstration on the Diabetes dataset. XAI techniques, namely LIME, SHAP, PDP, and decision tree, were used to explain how various input attributes contributed to the outcome of the model. This study found that the explanations provided by these models are not easily understandable for different users of the model, like doctors and patients, and need expertise. This study found that the potential of XAI in the medical domain is high as it increases trust in the AI model. This survey will motivate the researchers to build more XAI techniques that provide user-friendly explanations, especially for the less explored areas of medical data, such as biomedical signals and biomedical text.},
	author = {Gupta, Jyoti and Seeja, K. R.},
	date = {2024/09/01},
	date-added = {2025-01-10 17:09:36 +0100},
	date-modified = {2025-01-10 17:09:36 +0100},
	doi = {10.1007/s11831-024-10103-9},
	id = {Gupta2024},
	isbn = {1886-1784},
	journal = {Archives of Computational Methods in Engineering},
	number = {7},
	pages = {3977--4002},
	title = {A Comparative Study and Systematic Analysis of {XAI} Models and their Applications in Healthcare},
	url = {https://doi.org/10.1007/s11831-024-10103-9},
	volume = {31},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s11831-024-10103-9}}


@article{Guidotti2024,
	abstract = {Interpretable machine learning aims at unveiling the reasons behind predictions returned by uninterpretable classifiers. One of the most valuable types of explanation consists of counterfactuals. A counterfactual explanation reveals what should have been different in an instance to observe a diverse outcome. For instance, a bank customer asks for a loan that is rejected. The counterfactual explanation consists of what should have been different for the customer in order to have the loan accepted. Recently, there has been an explosion of proposals for counterfactual explainers. The aim of this work is to survey the most recent explainers returning counterfactual explanations. We categorize explainers based on the approach adopted to return the counterfactuals, and we label them according to characteristics of the method and properties of the counterfactuals returned. In addition, we visually compare the explanations, and we report quantitative benchmarking assessing minimality, actionability, stability, diversity, discriminative power, and running time. The results make evident that the current state of the art does not provide a counterfactual explainer able to guarantee all these properties simultaneously.},
	author = {Guidotti, Riccardo},
	date = {2024/09/01},
	date-added = {2025-01-23 16:31:34 +0100},
	date-modified = {2025-01-23 16:31:34 +0100},
	doi = {10.1007/s10618-022-00831-6},
	id = {Guidotti2024},
	isbn = {1573-756X},
	journal = {Data Mining and Knowledge Discovery},
	number = {5},
	pages = {2770--2824},
	title = {Counterfactual explanations and how to find them: literature review and benchmarking},
	url = {https://doi.org/10.1007/s10618-022-00831-6},
	volume = {38},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1007/s10618-022-00831-6}}


@article{Maragno2024-robustCE,
	abstract = { Counterfactual explanations (CEs) play an important role in detecting bias and improving the explainability of data-driven classification models. A CE is a minimal perturbed data point for which the decision of the model changes. Most of the existing methods can only provide one CE, which may not be achievable for the user. In this work, we derive an iterative method to calculate robust CEs (i.e., CEs that remain valid even after the features are slightly perturbed). To this end, our method provides a whole region of CEs, allowing the user to choose a suitable recourse to obtain a desired outcome. We use algorithmic ideas from robust optimization and prove convergence results for the most common machine learning methods, including decision trees, tree ensembles, and neural networks. Our experiments show that our method can efficiently generate globally optimal robust CEs for a variety of common data sets and classification models.History: Accepted by Andrea Lodi, Area Editor for Design \& Analysis of Algorithms---Discrete.Funding: This work was supported by the Nederlandse Organisatie voor Wetenschappelijk Onderzoek [Grant OCENW.GROOT.2019.015, Optimization for and with Machine Learning (OPTIMAL)].Supplemental Material: The online appendix is available at https://doi.org/10.1287/ijoc.2023.0153. },
	author = {Maragno, Donato and Kurtz, Jannis and R\"{o}ber, Tabea E. and Goedhart, Rob and Birbil, \c{S}. {\.I}lker and den Hertog, Dick},
	doi = {10.1287/ijoc.2023.0153},
	eprint = {https://doi.org/10.1287/ijoc.2023.0153},
	journal = {INFORMS Journal on Computing},
	number = {5},
	pages = {1316-1334},
	title = {Finding Regions of Counterfactual Explanations via Robust Optimization},
	url = {https://doi.org/10.1287/ijoc.2023.0153},
	volume = {36},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1287/ijoc.2023.0153}}

@misc{qualitative-study,
    author = {Tenny, S and Brannan, JM and Brannan GD},
title = {Qualitative Study},
year = {2022},
url = {https://www.ncbi.nlm.nih.gov/books/NBK470395/},
note = {Accessed: 2025-01-27}
}


@article{Dunwoodie2023,
	abstract = {Abstract Empirical research in the field of work and organisational psychology has typically adopted quantitative methods such as surveys or experiments. Comparatively less research has adopted qualitative methods such as interviewing. The aim and purpose of this article is to provide a practical guide for work and organisational psychology researchers, especially those more familiar with quantitative methods, to get started with qualitative interview approaches. The authors decided to focus on qualitative interviews as they are the most common method adopted in qualitative work and are often combined with other qualitative techniques such as participant observation or quantitative techniques such as surveys or experiments. This article looks at the strengths of adopting a qualitative interview design when conducting empirical research in the fields of work and organisational psychology, and the challenges faced in publishing such research. Finally, the article provides researchers and reviewers with guidelines for effectively executing, publishing and evaluating research adopting a qualitative interview design, and highlights exemplary articles in top journals that adopt qualitative interview designs.},
	author = {Dunwoodie, Karen and Macaulay, Luke and Newman, Alexander},
	doi = {https://doi.org/10.1111/apps.12414},
	eprint = {https://iaap-journals.onlinelibrary.wiley.com/doi/pdf/10.1111/apps.12414},
	journal = {Applied Psychology},
	keywords = {guidelines, interviews, qualitative research},
	number = {2},
	pages = {863-889},
	title = {Qualitative interviewing in the field of work and organisational psychology: Benefits, challenges and guidelines for researchers and reviewers},
	url = {https://iaap-journals.onlinelibrary.wiley.com/doi/abs/10.1111/apps.12414},
	volume = {72},
	year = {2023},
	bdsk-url-1 = {https://iaap-journals.onlinelibrary.wiley.com/doi/abs/10.1111/apps.12414},
	bdsk-url-2 = {https://doi.org/10.1111/apps.12414}}
