{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b749cc70",
   "metadata": {},
   "source": [
    "# Week 03 – Unboxing Methods\n",
    "\n",
    "In this notebook we'll be training a black-box model to the dataset which we will then explain using different post-hoc explanation methods.\n",
    "\n",
    "Make sure to install the required packages. You can use the lines below to set up your environment. You may also use the environment of last week and install the missing packages in there.\n",
    " \n",
    "```\n",
    "# navigate to parent directory using cd and activate environment\n",
    "cd <path-to-week3>\n",
    "\n",
    "# create and activate new environment\n",
    "# on Mac\n",
    "python3.11 -m venv week3_venv\n",
    "source week3_venv/bin/activate\n",
    "\n",
    "# on Windows\n",
    "python3.11 -m venv week3_venv\n",
    "#or \n",
    "py -3.11 -m venv week3_venv\n",
    "week3_venv\\Scripts\\activate\n",
    "\n",
    "# install packages\n",
    "pip install lime shap torch grad-cam\n",
    "\n",
    "# install jupyter kernel make environment available as kernel \n",
    "pip install ipykernel\n",
    "python -m ipykernel install --user --name=week3_venv\n",
    "\n",
    "# close environment\n",
    "deactivate\n",
    "```\n",
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e4b241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6909df",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Tabular data – Titanic dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0e93fa",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We are using the Statlog (German Credit Data) dataset (Dheeru Dua and Casey Graff. UCI machine learning repository, 2017. URL [http://archive.ics.uci.edu](http://archive.ics.uci.edu)). The German Credit dataset classifies people described by a set of 20 features as good or bad credit risk.\n",
    "\n",
    "Make sure to save the dataset in the parent directory or adjust the file path below.\n",
    "\n",
    "We are directly loading the pre-processed data sets. For some algorithms, we require the data to be in binary form. Hence, we have two versions of X: `X_train` and `X_test`, with continous features and one-hot encoded categorical features, and `X_train_bin` and `X_test_bin`, where all features have been one-hot encoded. For this, continuous features were first transformed into categories. Check the code in `./01_intro/data_prep.ipynb` for more details on pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248acf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train \n",
    "X_train = pd.read_csv('../datasets/titanic/encoded_titanic_X_train.csv')\n",
    "\n",
    "# X_test\n",
    "X_test = pd.read_csv('../datasets/titanic/encoded_titanic_X_test.csv')\n",
    "\n",
    "# y_train and y_test\n",
    "y_train = pd.read_csv('../datasets/titanic/titanic_y_train.csv')\n",
    "y_test = pd.read_csv('../datasets/titanic/titanic_y_test.csv')\n",
    "\n",
    "# take a look at the data\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2f395",
   "metadata": {},
   "source": [
    "## About the dataset\n",
    "\n",
    "- `Age` – age of a person in years (int)\n",
    "- `SibSp` – the number of siblings or spouse of a person **onboard** (int)\n",
    "- `Parch` – the number of parents or children of a person **onbard** (int)\n",
    "- `Fare` – ticket price (float)\n",
    "- `Sex` – sex of a person (categorical/binary)\n",
    "- `Embarked` – location where the traveler mounted from. There are three possible values — Southampton, Cherbourg, and Queenstown (categorical)\n",
    "- `Pclass` – passenger division into class 1, 2, and 3 (categorical)\n",
    "- `Survived` – whether person survived the sinking of the ship (binary). Less than 40% survived. This is the **outcome** to predict. \n",
    "\n",
    "\n",
    "From the original dataset and from preprocessing the data, we know the following about the **categorical features**:\n",
    "\n",
    "- `Sex` has two values `['female','male']`, which were encoded `[0,1]`, respectively. Then, after applying one-hot encoding, we have `Sex_1` which indicates `male` if 1, `female` otherwise.\n",
    "- `Embarked` has three values `['C', 'Q', 'S']`, which were encoded `[0,1,2]`, respectively. Hence,\n",
    "    - `Embarked_1 = 1` indicates `Q` \n",
    "    - `Embarked_2 = 1` indicates `S`\n",
    "    - `Embarked_1 = 0` and `Embarked_2 = 0` indicated `C`\n",
    "- `Pclass` has three values `[1,2,3]`, which were encoded `[0,1,2]`, respectively. Hence, after encoding, we have:\n",
    "    - `Pclass_1 = 1` indicates `2`\n",
    "    - `Pclass_2 = 1` indicates `3`\n",
    "    - `Pclass_1 = 0` and `Pclass_2 = 0` indicates `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e798643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save feature names\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "# create a dataframe to save y values in \n",
    "y_results = pd.DataFrame()\n",
    "y_results['y_test'] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60564ca",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e7935",
   "metadata": {},
   "source": [
    "# 1. Training a black-box model\n",
    "\n",
    "We will train a complex, uninterpretable model to the dataset. For the sake of this tutorial, we've chosen to train a Gradient Boosting classifier. We'll use `scikit-learn`'s implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50019a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b684b00b",
   "metadata": {},
   "source": [
    "We are performig grid search and 5-fold cross validation to determine the best combinations of values for the parameters `n_estimators` and `max_depth`, from the set {100,150,200,250,300} and {2,3,5}, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e85de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing grid search to find the best parameter values\n",
    "pgrid = {'n_estimators':[100,150,200,250,300],\n",
    "        'max_depth':[2,3,5]}\n",
    "\n",
    "gb = GradientBoostingClassifier(random_state=0)\n",
    "gcv = GridSearchCV(estimator=gb, param_grid=pgrid, n_jobs=1, cv=5, verbose=1, refit=True)\n",
    "gcv_fit = gcv.fit(X_train, y_train)\n",
    "gb = gcv_fit.best_estimator_\n",
    "y_results['gb_pred'] = gb.predict(X_test)\n",
    "y_results['gb_pred_prob'] = gb.predict_proba(X_test)[:, 1]\n",
    "print(gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cafcb55",
   "metadata": {},
   "source": [
    "## inspect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d5522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = pd.crosstab(y_results['y_test'], y_results['gb_pred'])\n",
    "print (\"Confusion matrix : \\n\", cm)\n",
    "\n",
    "print('\\nAccuracy  = %.4f' % accuracy_score(y_results['y_test'], y_results['gb_pred']))\n",
    "print('F1 score  = %.4f' % f1_score(y_results['y_test'], y_results['gb_pred']))\n",
    "num_leaves = sum(tree.tree_.n_leaves for tree in gb.estimators_.reshape(-1))           \n",
    "print('Number of rules = %.0f' % (num_leaves))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef0db50",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 1.1 – Which parameters seem to have worked best according to the grid search?**\n",
    "\n",
    "...\n",
    "\n",
    "**Q 1.2 – Evaluate the model in terms of performance and interpretability.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fd915",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# 2. LIME\n",
    "\n",
    "Local interpretable model-agnostic explanations (LIME) was proposed by [Ribeiro et al. (2016)](https://dl.acm.org/doi/abs/10.1145/2939672.2939778) and implemented in the package `lime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b76a48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import lime_tabular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21282606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate explainer instance\n",
    "explainer = lime_tabular.LimeTabularExplainer(np.array(X_train), mode='classification',\n",
    "                                              training_labels=list(y_train['Survived'].unique()),\n",
    "                                              feature_names = feature_names,random_state=0,\n",
    "                                              categorical_features=[4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9cbf05",
   "metadata": {},
   "source": [
    "### LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49177b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 30\n",
    "exp_gb = explainer.explain_instance(np.array(X_test.iloc[i,:]), gb.predict_proba, num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4efada",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True and predicted values for sample at index {i}:')\n",
    "print(round(y_results.loc[i,:],2))\n",
    "print('\\n')\n",
    "exp_gb.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d08290",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 2.1 – Take a look at the local explanation for sample with index 30 of the test set. How would you interpret this explanation?.**\n",
    "\n",
    "The sample got predicted as 0 (not survived), with a probablity of 0.69. We can see that the feature with the highest impact is `Sex_1`, for which the sample has a value of 1, so our sample is male, which reduces their chance of survival quite a lot. On the other hand, this person didn't travel third class (Pclass_2 = 0) and paid a relatively higher fare, which both increases his chance of survival a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be484b61",
   "metadata": {},
   "source": [
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e8de636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime import submodular_pick\n",
    "\n",
    "sp_exp = submodular_pick.SubmodularPick(explainer, \n",
    "                                        np.array(X_test),\n",
    "                                        gb.predict_proba, \n",
    "                                        num_features=5,\n",
    "                                        num_exps_desired=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2d8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "[exp.show_in_notebook() for exp in sp_exp.sp_explanations]\n",
    "print('SP-LIME Explanations.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0990b322",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 2.2 – Explain what we're seeing here as global explanation.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eff623",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e73c0",
   "metadata": {},
   "source": [
    "# 3. SHAP\n",
    "\n",
    "Based on Shapley values from coalitional game theory, SHAP is a model-agnostic method that assigns feature contributions and was introduced by [Lundberg et al. (2017)](https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html). Besides LIME it belongs to the most well known post-hoc explanation methods and is implemented in the package `shap`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d6c0d2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "explainer = shap.TreeExplainer(gb, X_train)\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c5fe8",
   "metadata": {},
   "source": [
    "### GLOBAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9272a0",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 3.1 – Explain what the summary plot shows and how it can be interpreted.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9280db22",
   "metadata": {},
   "source": [
    "### LOCAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559a4395",
   "metadata": {},
   "source": [
    "There are multiple ways of displaying the feature importances produced by SHAP. Here we show waterfall and force plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789c607f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True and predicted values for sample at index {i}:')\n",
    "print(round(y_results.loc[i,:],2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shap\n",
    "print('----\\nSHAP waterfall plot\\n----')\n",
    "shap.plots.waterfall(shap_values[30,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2b8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----\\nSHAP force plot\\n----')\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value,\n",
    "                              shap_values[i,:].values,\n",
    "                              X_test.iloc[i,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf6db5c",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 3.1 – Take a look at the waterfall plot and the force plot. Describe what they show, whether they differ in the information they provide and how you can interpret both.**\n",
    "\n",
    "...\n",
    "\n",
    "**Q 3.2 – Is there a way to display only some of the features, let's say the top 5? If yes, how?**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512cfb0b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# Image data – Fashion MNIST data\n",
    "\n",
    "# 1. SHAP on images\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fb3e5e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "11854ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('../datasets/fashion_mnist/fashion_mnist_X_train.csv', dtype = np.float32)\n",
    "y_train = pd.read_csv('../datasets/fashion_mnist/fashion_mnist_y_train.csv', dtype = np.float32)\n",
    "X_test = pd.read_csv('../datasets/fashion_mnist/fashion_mnist_X_test.csv', dtype = np.float32)\n",
    "y_test = pd.read_csv('../datasets/fashion_mnist/fashion_mnist_y_test.csv',dtype = np.float32)\n",
    "\n",
    "X_train = X_train.values/255\n",
    "y_train = y_train.values.flatten()\n",
    "X_test = X_test.values/255\n",
    "y_test = y_test.values.flatten()\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "y_train = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "X_test = torch.from_numpy(X_test)\n",
    "y_test = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "batch_size = 128\n",
    "num_epochs = 5\n",
    "\n",
    "train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebeca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {\n",
    "    0:'T-shirt/top',\n",
    "    1:'Trouser',\n",
    "    2:'Pullover',\n",
    "    3:'Dress',\n",
    "    4:'Coat',\n",
    "    5:'Sandal',\n",
    "    6:'Shirt',\n",
    "    7:'Sneaker',\n",
    "    8:'Bag',\n",
    "    9:'Ankle boot'\n",
    "}\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "images, labels = batch\n",
    "\n",
    "# choose random indices for the images\n",
    "random.seed(0)\n",
    "random_indices = random.sample(range(len(images)), 5)\n",
    "\n",
    "# loop through the random indices and save the corresponding images & labels\n",
    "imgs, lbls = [], []\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # Access the image from the dataset along with its label\n",
    "    imgs.append(images[idx])\n",
    "    lbls.append(labels[idx])\n",
    "\n",
    "# plot example images\n",
    "fig, axes = plt.subplots(1,5, figsize=(8,5))   \n",
    "for i, img in enumerate(imgs):\n",
    "    test = img.view(-1, 1, 28, 28)\n",
    "    axes[i].imshow(-test.squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f'{labels_dict[lbls[i].item()]}')\n",
    "    axes[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce201d5",
   "metadata": {},
   "source": [
    "## train a convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7e9c5133",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 10) \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9c0458d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# initialize model \n",
    "model = CNNModel()\n",
    "\n",
    "# specifiy error as cross entropy loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea7d73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "#     print('Epoch: {}'.format(epoch))\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        train = Variable(images.view(-1, 1, 28, 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad() # clear gradients\n",
    "        outputs = model(train) # forward propagation\n",
    "        loss = error(outputs, labels) # calculate softmax and cross entropy loss\n",
    "        loss.backward() # calculating gradients\n",
    "        optimizer.step() # update parameters\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, i * len(images), len(train_loader.dataset),\n",
    "                100. * i / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "605fd2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save model\n",
    "# PATH = '../03_unboxing/models/fashion_mnist_net_trained.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaacb398",
   "metadata": {},
   "source": [
    "## test performance on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4695b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the trained model if necessary\n",
    "# PATH = '../03_unboxing/models/fashion_mnist_net_trained.pth'\n",
    "# model = CNNModel()\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e68713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test dataset\n",
    "model.eval()\n",
    "correct = 0\n",
    "test_loss = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        test = Variable(images.view(-1, 1, 28, 28))\n",
    "        outputs = model(test) # forward propagation\n",
    "        test_loss += error(outputs, labels.flatten()).item()  # sum up batch loss\n",
    "        predicted = torch.max(outputs.data, 1)[1] # get predicted label from the maximum value\n",
    "        probs = F.softmax(outputs) # get all predicted probabilities\n",
    "        pred_probs = probs[0][predicted] # get probability of predicted label\n",
    "        correct += (predicted == labels).sum() # total correct predictions\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5757378",
   "metadata": {},
   "source": [
    "### inspect prediction accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f1b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        # images, labels = data\n",
    "        test = Variable(images.view(-1, 1, 28, 28))\n",
    "        outputs = model(test)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[label.item()] += 1\n",
    "            total_pred[label.item()] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classes[classname]:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa9baa6",
   "metadata": {},
   "source": [
    "## inspect SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5c904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap \n",
    "\n",
    "batch = next(iter(test_loader))\n",
    "images, labels = batch\n",
    "images = images.view(-1, 1, 28, 28)\n",
    "\n",
    "# choose random indices for the images\n",
    "random.seed(0)\n",
    "random_indices = random.sample(range(len(images)), 10)\n",
    "\n",
    "# loop through the random indices and save the corresponding images & labels\n",
    "imgs, lbls = [], []\n",
    "for i, idx in enumerate(random_indices):\n",
    "    # get image from the dataset along with its label\n",
    "    imgs.append(images[idx])\n",
    "    lbls.append(labels[idx])\n",
    "\n",
    "background = images[:100]\n",
    "test_images= images[100:110]\n",
    "test_labels= labels[100:110]\n",
    "\n",
    "e = shap.DeepExplainer(model, background)\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "    test = imgs[i].view(-1, 1, 28, 28)\n",
    "    shap_values = e.shap_values(test)\n",
    "\n",
    "    test_numpy = np.transpose(test.numpy(), (0, 2, 3, 1))\n",
    "\n",
    "    out = model(test) # forward propagation\n",
    "    predicted = torch.max(out.data, 1)[1] # get predicted label from the maximum value\n",
    "    probs = F.softmax(out).detach() # get probabilities for each class\n",
    "    pred_probs = probs[0][predicted] # get predicted probability for predicted label\n",
    "\n",
    "    print('--------\\n')\n",
    "    print(\"Actual Class: {}, Predicted Class: {}, Predicted Probability: {:.4f}\\n\".format(\n",
    "        labels_dict[lbls[i].item()],\n",
    "        labels_dict[predicted.item()],\n",
    "        pred_probs.item()\n",
    "    ))\n",
    "\n",
    "    for cls in labels_dict.keys():\n",
    "        print('Prob for class {}: {:.4f}'.format(\n",
    "            labels_dict[cls],\n",
    "            probs[0][cls]\n",
    "        ))\n",
    "\n",
    "    shap_numpy = [np.transpose(shap_values[0, ..., i], (1, 2, 0)).reshape(1, 28, 28, 1) for i in range(shap_values.shape[-1])]\n",
    "    shap.image_plot(shap_numpy, -test_numpy, labels=list(labels_dict.values()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4715f6a",
   "metadata": {},
   "source": [
    "**If you have older shap versions (e.g. 0.44.1), try the below code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3669408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap \n",
    "\n",
    "# batch = next(iter(test_loader))\n",
    "# images, labels = batch\n",
    "# images = images.view(-1, 1, 28, 28)\n",
    "\n",
    "# # choose random indices for the images\n",
    "# random.seed(0)\n",
    "# random_indices = random.sample(range(len(images)), 10)\n",
    "\n",
    "# # loop through the random indices and save the corresponding images & labels\n",
    "# imgs, lbls = [], []\n",
    "# for i, idx in enumerate(random_indices):\n",
    "#     # get image from the dataset along with its label\n",
    "#     imgs.append(images[idx])\n",
    "#     lbls.append(labels[idx])\n",
    "\n",
    "# background = images[:100]\n",
    "# test_images= images[100:110]\n",
    "# test_labels= labels[100:110]\n",
    "\n",
    "# e = shap.DeepExplainer(model, background)\n",
    "\n",
    "# # for i in range(len(test_images)):\n",
    "# for i in range(len(imgs)):\n",
    "#     test = imgs[i].view(-1, 1, 28, 28)\n",
    "#     shap_values = e.shap_values(test)\n",
    "\n",
    "#     shap_numpy = [np.swapaxes(np.swapaxes(s, 1, -1), 1, 2) for s in shap_values]\n",
    "#     test_numpy = np.swapaxes(np.swapaxes(test.numpy(), 1, -1), 1, 2)\n",
    "    \n",
    "#     out = model(test) # forward propagation\n",
    "#     predicted = torch.max(out.data, 1)[1] # get predicted label from the maximum value\n",
    "#     probs = F.softmax(out).detach() # get probabilities for each class\n",
    "#     # pred_probs = probs[range(len(predicted)), predicted]  \n",
    "#     pred_probs = probs[0][predicted] # get predicted probability for predicted label\n",
    "    \n",
    "#     print('--------\\n')\n",
    "#     print(\"Actual Class: {}, Predicted Class: {}, Predicted Probability: {:.4f}\\n\".format(\n",
    "# #         labels_dict[test_labels[i].item()],\n",
    "#         labels_dict[lbls[i].item()],\n",
    "#         labels_dict[predicted.item()],\n",
    "#         pred_probs.item()\n",
    "        \n",
    "#     ))\n",
    "    \n",
    "#     for cls in labels_dict.keys():\n",
    "#         print('Prob for class {}: {:.4f}'.format(\n",
    "#             labels_dict[cls],\n",
    "#              probs[0][cls]))\n",
    "    \n",
    "#     shap.image_plot(shap_numpy, -test_numpy, labels = list(labels_dict.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2845c2",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 1.1 Explain the output. What is displayed and what does it mean?**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f073c1",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d100a6",
   "metadata": {},
   "source": [
    "# 2. Gradient-based methods\n",
    "\n",
    "In gradient-based attribution methods, the gradients of the output (logits or soft-max probabilities) with respect to the extracted features or the input are calculated via backpropagation and are used to estimate attribution scores. Several variations of gradient-based approaches exist attempting to reduce the noise in the resulting attribution maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3781a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288de2a1",
   "metadata": {},
   "source": [
    "For loading the data and training the network, we loosely follow [this tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3eda00",
   "metadata": {},
   "source": [
    "## Load CIFAR10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3f8160b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "# load data\n",
    "trainset = torchvision.datasets.CIFAR10(root='../datasets', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../datasets', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "classes = {\n",
    "    0:'plane',\n",
    "    1:'car',\n",
    "    2:'bird',\n",
    "    3:'cat',\n",
    "    4:'deer',\n",
    "    5:'dog',\n",
    "    6:'frog',\n",
    "    7:'horse',\n",
    "    8:'ship',\n",
    "    9:'truck'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b09cf0",
   "metadata": {},
   "source": [
    "### inspecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ddf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show an image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "## visualise some examples fromthe training data\n",
    "# print labels\n",
    "print([classes[labels[j].item()] for j in range(batch_size)])\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3cb8f3",
   "metadata": {},
   "source": [
    "## Training a neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "1e3d10c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab2f39",
   "metadata": {},
   "source": [
    "### training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6693e856",
   "metadata": {},
   "source": [
    "### &#9888;&#65039; &#9888;&#65039; If the training process takes too long on your machine, you can load the trained model from the repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "1d47630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 1\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "# define your model\n",
    "model = Net()\n",
    "\n",
    "# define the optimizer and loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a493e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        if i % 2000 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch + 1, i * len(data), len(trainloader.dataset),\n",
    "                100. * i / len(trainloader), loss.item()))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68053e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the trained model if desired\n",
    "# PATH = '../03_unboxing/models/cifar_net_trained.pth'\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16520275",
   "metadata": {},
   "source": [
    "### evaluating on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a0c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the trained model if necessary\n",
    "# PATH = '../03_unboxing/models/cifar_net_trained.pth'\n",
    "# model = Net()\n",
    "# model.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64a0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "# since we're not training, we don't need to calculate the gradients for our outputs\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = model(images)\n",
    "        # the class with the highest value is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    correct, len(testloader.dataset),\n",
    "    100. * correct / len(testloader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab39e2a",
   "metadata": {},
   "source": [
    "### inspect prediction accuracy per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb079d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[label.item()] += 1\n",
    "            total_pred[label.item()] += 1\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classes[classname]:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a61fc6",
   "metadata": {},
   "source": [
    "## Applying GradCAM\n",
    "\n",
    "We are using the implementation from [this package](https://github.com/jacobgil/pytorch-grad-cam)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ff1c59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer = model.conv2 # layer to visualize (last convolutional layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3483783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM\n",
    "import cv2\n",
    "\n",
    "# initialize the GradCAM instance with your model\n",
    "gradcam = GradCAM(model=model, target_layers=[target_layer])\n",
    "\n",
    "# get some images\n",
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# get predicted values\n",
    "outputs = model(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "probs = F.softmax(outputs, dim=1)\n",
    "# probs\n",
    "\n",
    "# compute the GradCAM heatmaps\n",
    "cam_list = []\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image.requires_grad_()\n",
    "    grayscale_cam = gradcam(input_tensor=image.unsqueeze(0))\n",
    "    cam_list.append(grayscale_cam)\n",
    "    \n",
    "    image = image.cpu().permute(1, 2, 0).detach().numpy()\n",
    "    heatmap = cam_list[i][0]  # assuming you only have one heatmap for each image\n",
    "    \n",
    "    # resize heatmap to match image dimensions\n",
    "    heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))\n",
    "    \n",
    "    # convert heatmap to 8-bit unsigned integer (CV_8UC1) (required to apply colormap)\n",
    "    heatmap = (heatmap * 255).astype('uint8')\n",
    "    heatmap = cv2.applyColorMap(1-heatmap, cv2.COLORMAP_JET) # apply colormap\n",
    "    # normalize heatmap\n",
    "    vmin = heatmap.min()\n",
    "    vmax = heatmap.max()\n",
    "    heatmap = (heatmap - vmin)/(vmax - vmin)\n",
    "\n",
    "    # overlay heatmap on image\n",
    "    overlaid_img = heatmap * 0.5 + image * 0.5   # adjust the weight for overlay\n",
    "\n",
    "    print('-------------------------------\\n')\n",
    "    print(f'Ground Truth: {classes[labels[i].item()]}')\n",
    "    print(f'Predicted: {classes[predicted[i].item()]}\\n')\n",
    "    \n",
    "    for cls in classes.keys():\n",
    "        print('Prob for class {}: {:.4f}'.format(\n",
    "            classes[cls],\n",
    "             probs[i][cls]))\n",
    "        \n",
    "    im_tensor = torch.tensor(image.transpose((2, 0, 1)))\n",
    "    heat_tensor = torch.tensor(heatmap.transpose((2, 0, 1)))\n",
    "    overlay = torch.tensor(overlaid_img.transpose((2,0,1)))\n",
    "    imshow(torchvision.utils.make_grid([im_tensor, heat_tensor, overlay]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8aa7f0",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "**Q 2.1 Explain the output.**\n",
    "\n",
    "...\n",
    "\n",
    "**Q 2.2 Evaluate the explanations given by Gradcam.**\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388693ad",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "week3_venv",
   "language": "python",
   "name": "week3_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
